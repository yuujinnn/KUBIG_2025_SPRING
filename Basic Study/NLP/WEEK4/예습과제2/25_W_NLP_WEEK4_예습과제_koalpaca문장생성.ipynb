{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsnGgar7gx83"
   },
   "source": [
    "4주차 예습과제에서는 5주차 내용인 GPT의 생성 능력을 맛보고자 합니다. 사용할 모델은 koalpaca라는 모델인데, 한국어 버전의 alpaca 모델입니다.\n",
    "\n",
    "alpaca는 llama라는 모델을 instruction fine-tuning 해서 사람의 지시를 잘 따르는 모델입니다.\n",
    "\n",
    "llama라는 모델은 LLaMA-13B의 경우 GPT-3보다 10배이상 작음에도 불구하고 대부분의 평가서 GPT-3보다 우수한 성능을 보이며, 더 나아가 LLaMA-65B의 경우 대부분의 벤치마크에서 Chinchilla, Gopher, GPT-3, PaLM와 유사하거나 더 뛰어난 결과를 보이는 훌륭한 모델입니다.\n",
    "\n",
    "그러면 지금부터 koalpaca로 생성 모델을 한번 경험해보도록 하겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7crJC7X_eb1v"
   },
   "outputs": [],
   "source": [
    "# GPU 켜져 있는지 확인\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o186Sm7Uef5V"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgCKqAXMekYz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syg88baNin2b"
   },
   "source": [
    "peft는 parameter efficient fine-tuning의 약자로, 큰 모델을 코랩에서 돌릴 수 있도록 쪼개서 만든 것입니다.\n",
    "\n",
    "오래 걸립니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlZv-twPerjy"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYf0UQ3fetzP"
   },
   "outputs": [],
   "source": [
    "peft_model_id = \"beomi/qlora-koalpaca-polyglot-12.8b-50step\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViGa1Tvyi8fI"
   },
   "source": [
    "next token generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHacSUa3fgz0"
   },
   "outputs": [],
   "source": [
    "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO4qeMpZi54_"
   },
   "source": [
    " QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JBJKYJcfLuX"
   },
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    q = f\"### 질문: {x}\\n\\n### 답변:\"\n",
    "    # print(q)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            q,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'),\n",
    "        max_new_tokens=50,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(tokenizer.decode(gened[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55GdkP9afi6U"
   },
   "outputs": [],
   "source": [
    "gen('건강하게 살기 위한 세 가지 방법은?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjacO_6njJWR"
   },
   "source": [
    "top_k와 같은 parameter를 조절하면서 실험해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JaD2vpPfkuU"
   },
   "outputs": [],
   "source": [
    "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=50)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
