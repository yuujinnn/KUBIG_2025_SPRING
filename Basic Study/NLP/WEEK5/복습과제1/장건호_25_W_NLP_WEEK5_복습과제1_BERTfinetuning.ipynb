{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkUGTqixRWo"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66"
      },
      "source": [
        "## 1.1. Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "GPU 사용 가능 여부 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "outputId": "a416693a-558a-408f-f646-e93816d204c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA GeForce RTX 4060 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "## 1.2. Installing the Hugging Face Library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_N2UDLevYWn"
      },
      "source": [
        "현재 Hugging Face 라이브러리는 BERT 작업을 위한 가장 널리 사용되는 인터페이스입니다. 사전 훈련된 다양한 transformer 모델을 지원하는 것 외에도 라이브러리에는 특정 작업에 적합한 이러한 모델의 사전 구축된 수정 사항도 포함되어 있습니다. 예를 들어, 본 과제에서는 `BertForSequenceClassification`을 사용합니다.\n",
        "\n",
        "라이브러리에는 토큰 분류, 질문 답변, 다음 문장 예측 등을 위한 작업별 클래스도 포함되어 있습니다. 이러한 사전 구축된 클래스를 사용하면 목적에 맞게 BERT를 수정하는 프로세스가 단순화됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "# 2. Loading CoLA Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ZKxKc04Btk"
      },
      "source": [
        "단일 문장 분류에는 [CoLA(The Corpus of Linguistic Acceptability)](https://nyu-mll.github.io/CoLA/) 데이터세트를 사용하겠습니다. 문법적으로 정확하거나 틀린 것으로 표시된 문장 데이터셋입니다. 2018년 5월에 처음 공개되었으며 BERT와 같은 모델이 평가되는 \"GLUE 벤치마크\"에 포함된 테스트 중 하나입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "## 2.1. Download & Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZNVW6xd0T0X"
      },
      "source": [
        "`wget` 패키지로 데이터를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "outputId": "375ebf04-4b06-4de2-b8f1-a01e50282e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in c:\\users\\user\\anaconda3\\lib\\site-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "outputId": "9aaae2ae-ae2d-4bf6-f6a0-142f6f10206d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ],
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yv-tNv20dnH",
        "outputId": "829fb6ce-af5f-4e97-9544-342fa51e8b38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "# Unzip\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_"
      },
      "source": [
        "## 2.2. Parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "outputId": "fe1216c4-6633-4c8e-99ab-b78464236722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3180</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>She always wore purple dresses.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4821</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I don't know that to agree with him or not.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2840</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Carrie touched the stick against the cat.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1891</th>\n",
              "      <td>r-67</td>\n",
              "      <td>0</td>\n",
              "      <td>??</td>\n",
              "      <td>John has a hole in his quilt's upper right-han...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7750</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Which god the statue?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fred watered the plants flat.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6976</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I saw and he chops.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8069</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I asked if Medea poisoned Jason.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6768</th>\n",
              "      <td>m_02</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>When never had Sir Thomas been so offended, Mr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>The boys were made a good mother.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "3180            l-93      1         NaN   \n",
              "4821            ks08      0           *   \n",
              "2840            l-93      0           *   \n",
              "1891            r-67      0          ??   \n",
              "7750            ad03      0           *   \n",
              "6               gj04      1         NaN   \n",
              "6976            m_02      1         NaN   \n",
              "8069            ad03      1         NaN   \n",
              "6768            m_02      0           *   \n",
              "735             bc01      0           *   \n",
              "\n",
              "                                               sentence  \n",
              "3180                    She always wore purple dresses.  \n",
              "4821        I don't know that to agree with him or not.  \n",
              "2840          Carrie touched the stick against the cat.  \n",
              "1891  John has a hole in his quilt's upper right-han...  \n",
              "7750                              Which god the statue?  \n",
              "6                         Fred watered the plants flat.  \n",
              "6976                                I saw and he chops.  \n",
              "8069                   I asked if Medea poisoned Jason.  \n",
              "6768  When never had Sir Thomas been so offended, Mr...  \n",
              "735                   The boys were made a good mother.  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"C:/Users/User/Desktop/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWzpPi92UAH"
      },
      "source": [
        "`sentence` 와 `label`\b만 남기겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blqIvQaQncdJ",
        "outputId": "5a651244-602b-438f-da0c-86d0fab1a34c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3660</th>\n",
              "      <td>John seems know about the bananas.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4417</th>\n",
              "      <td>Kim must bakes a cake.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8160</th>\n",
              "      <td>I persuaded there to be a problem.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2479</th>\n",
              "      <td>Kelly buttered the bread with butter.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8511</th>\n",
              "      <td>Peter is those pigs.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   sentence  label\n",
              "3660     John seems know about the bananas.      0\n",
              "4417                 Kim must bakes a cake.      0\n",
              "8160     I persuaded there to be a problem.      0\n",
              "2479  Kelly buttered the bread with butter.      0\n",
              "8511                   Peter is those pigs.      0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "outputs": [],
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "이 섹션에서는 데이터 세트를 BERT가 학습할 수 있는 형식으로 변환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5"
      },
      "source": [
        "## 3.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWOPOyWghJp2"
      },
      "source": [
        "텍스트를 BERT에 공급하려면 텍스트를 토큰으로 분할한 다음 이러한 토큰을 토크나이저 어휘의 인덱스에 매핑해야 합니다.\n",
        "\n",
        "토큰화는 BERT에 포함된 토크나이저에 의해 수행되어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z474sSC6oe7A",
        "outputId": "1bdd65ba-a73e-4ac5-b984-f47a21ec4f88",
        "colab": {
          "referenced_widgets": [
            "9386ea8f91484da09f9efe500115d9f9",
            "53a0f84f9a5b44ed8603301a2f7c0149",
            "debd4753f1f74ba484a93d8afddcca28",
            "3456b6ba9f7440febff1f3deccf8ecda"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9386ea8f91484da09f9efe500115d9f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53a0f84f9a5b44ed8603301a2f7c0149",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "debd4753f1f74ba484a93d8afddcca28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3456b6ba9f7440febff1f3deccf8ecda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLIbudgfh6F0",
        "outputId": "54703608-d3d5-41f6-9551-56410cbc71c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeNIc4auFUdF"
      },
      "source": [
        "실제로 모든 문장을 변환할 때 `tokenize`와 `convert_tokens_to_ids`를 별도로 호출하는 대신 `tokenize.encode` 함수를 사용하여 두 단계를 모두 처리합니다.\n",
        "\n",
        "하지만 그렇게 하기 전에 BERT의 형식 요구 사항 중 일부에 대해 살펴보겠습니다다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKGCCh8izww"
      },
      "source": [
        "## 3.2. Required Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDcqNlvVhL5W"
      },
      "source": [
        "위의 코드에는 여기서 살펴볼 몇 가지 필수 형식 지정 단계가 생략되었습니다.\n",
        "\n",
        "\n",
        "우리는 다음을 수행해야 합니다.\n",
        "1. 각 문장의 시작과 끝 부분에 특수 토큰을 추가.\n",
        "2. 모든 문장을 하나의 일정한 길이로 채우고 자릅니다.\n",
        "3. \"attention mask\"를 사용하여 실제 토큰과 패딩 토큰을 명시적으로 구별합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mceWWOjZnw"
      },
      "source": [
        "### Special Tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykk0P9JiKtVe"
      },
      "source": [
        "**`[SEP]`**\n",
        "\n",
        "모든 문장 끝에 특수 `[SEP]` 토큰을 추가해야 합니다.\n",
        "\n",
        "이 토큰은 BERT에 두 개의 별도 문장이 제공됨을 알립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C9objaKu8f"
      },
      "source": [
        "**`[CLS]`**\n",
        "\n",
        "분류 작업을 위해서는 모든 문장의 시작 부분에 특수 `[CLS]` 토큰을 추가해야 합니다.\n",
        "\n",
        "이 토큰은 특별한 의미를 갖습니다. BERT는 12개의 Transformer 레이어로 구성됩니다. 각 transformer는 토큰 임베딩 목록을 가져와 출력에 동일한 수의 임베딩을 생성합니다.\n",
        "\n",
        "![Illustration of CLS token purpose](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n",
        "\n",
        "최종(12번째) transformer의 출력에서 *classifier는 *첫 번째 임베딩([CLS] 토큰에 해당)만 사용합니다*.\n",
        "\n",
        "또한 BERT는 분류를 위해 이 [CLS] 토큰만 사용하도록 훈련되었기 때문에 모델이 분류 단계에 필요한 모든 것을 단일 768 값 임베딩 벡터로 인코딩하도록 되었습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u51v0kFxeteu"
      },
      "source": [
        "### Sentence Length & Attention Mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNuwqZVK3T6"
      },
      "source": [
        "BERT에는 두 가지 제약 조건이 있습니다.\n",
        "1. 모든 문장은 고정된 단일 길이로 채워지거나 잘려야 합니다.\n",
        "2. 최대 문장 길이는 512 토큰입니다.\n",
        "\n",
        "패딩은 BERT 어휘의 인덱스 0에 있는 특수 `[PAD]` 토큰을 사용하여 수행됩니다. 아래 그림은 8개 토큰의 \"MAX_LEN\"에 대한 패딩을 보여줍니다.\n",
        "\n",
        "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n",
        "\n",
        "\"attention mask\"는 단순히 패딩되는 토큰과 패딩되지 않는 토큰을 나타내는 1과 0의 배열입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ"
      },
      "source": [
        "## 3.2. Sentences to IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M296yz577fV"
      },
      "source": [
        "`tokenizer.encode` 함수는 여러 단계를 결합합니다:\n",
        "1. 문장을 토큰으로 나눕니다.\n",
        "2. 특수 `[CLS]` 및 `[SEP]` 토큰을 추가합니다.\n",
        "3. 토큰을 해당 ID에 매핑합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "outputId": "42cb79b3-cc57-4ea5-cfe2-e01b9afa9bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwCKszh6ych"
      },
      "source": [
        "## 3.3. Padding & Truncating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytsw1oIfnX0"
      },
      "source": [
        "시퀀스를 모두 채우고 잘라서 길이가 모두 'MAX_LEN'이 되도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqiWTDrn_nGB"
      },
      "source": [
        "First, what's the maximum sentence length in our dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhUZO9vc_l6T",
        "outputId": "fd930b5d-e75e-48e4-95b6-8e3cd3cfc25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ]
        }
      ],
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YvIyRyJRXBY",
        "outputId": "fde59b52-4447-425e-bce5-6eb00b5ed110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting absl-py (from keras)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
            "     ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 48.6/48.6 kB 2.6 MB/s eta 0:00:00\n",
            "Collecting ml-dtypes (from keras)\n",
            "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
            "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------- ----------------------- 0.5/1.3 MB 11.1 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 1.1/1.3 MB 11.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.3/1.3 MB 10.3 MB/s eta 0:00:00\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 133.7/133.7 kB 7.7 MB/s eta 0:00:00\n",
            "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
            "   ---------------------------------------- 0.0/210.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 210.9/210.9 kB 6.5 MB/s eta 0:00:00\n",
            "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
            "   ---------------------------------------- 0.0/299.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 299.6/299.6 kB 9.0 MB/s eta 0:00:00\n",
            "Installing collected packages: namex, optree, ml-dtypes, absl-py, keras\n",
            "Successfully installed absl-py-2.1.0 keras-3.8.0 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT35zg1oRXBY",
        "outputId": "1b0c0397-9412-4775-aee8-d5eeb8d371fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
            "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
            "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
            "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
            "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
            "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
            "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.5/390.3 MB 11.1 MB/s eta 0:00:36\n",
            "   ---------------------------------------- 1.1/390.3 MB 11.5 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 1.6/390.3 MB 11.5 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 2.2/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 2.7/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 3.3/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 3.8/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 4.3/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "   ---------------------------------------- 4.9/390.3 MB 11.5 MB/s eta 0:00:34\n",
            "    --------------------------------------- 5.4/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "    --------------------------------------- 6.0/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "    --------------------------------------- 6.5/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "    --------------------------------------- 7.1/390.3 MB 11.6 MB/s eta 0:00:34\n",
            "    --------------------------------------- 7.6/390.3 MB 11.6 MB/s eta 0:00:33\n",
            "    --------------------------------------- 8.2/390.3 MB 11.6 MB/s eta 0:00:33\n",
            "    --------------------------------------- 8.5/390.3 MB 11.8 MB/s eta 0:00:33\n",
            "    --------------------------------------- 8.6/390.3 MB 11.0 MB/s eta 0:00:35\n",
            "    --------------------------------------- 8.8/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "    --------------------------------------- 9.2/390.3 MB 10.3 MB/s eta 0:00:38\n",
            "    --------------------------------------- 9.6/390.3 MB 10.4 MB/s eta 0:00:37\n",
            "   - -------------------------------------- 10.2/390.3 MB 10.5 MB/s eta 0:00:37\n",
            "   - -------------------------------------- 10.7/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 11.3/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 11.8/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 12.4/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 13.0/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 13.5/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 14.1/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 14.6/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 15.2/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 15.7/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 16.3/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 16.8/390.3 MB 10.6 MB/s eta 0:00:36\n",
            "   - -------------------------------------- 17.0/390.3 MB 10.2 MB/s eta 0:00:37\n",
            "   - -------------------------------------- 17.0/390.3 MB 9.9 MB/s eta 0:00:38\n",
            "   - -------------------------------------- 17.1/390.3 MB 9.5 MB/s eta 0:00:40\n",
            "   - -------------------------------------- 17.2/390.3 MB 9.1 MB/s eta 0:00:42\n",
            "   - -------------------------------------- 17.3/390.3 MB 8.8 MB/s eta 0:00:43\n",
            "   - -------------------------------------- 17.4/390.3 MB 8.7 MB/s eta 0:00:43\n",
            "   - -------------------------------------- 17.5/390.3 MB 8.4 MB/s eta 0:00:45\n",
            "   - -------------------------------------- 17.6/390.3 MB 8.0 MB/s eta 0:00:47\n",
            "   - -------------------------------------- 17.7/390.3 MB 7.8 MB/s eta 0:00:48\n",
            "   - -------------------------------------- 17.7/390.3 MB 7.5 MB/s eta 0:00:50\n",
            "   - -------------------------------------- 17.8/390.3 MB 7.4 MB/s eta 0:00:51\n",
            "   - -------------------------------------- 17.9/390.3 MB 7.1 MB/s eta 0:00:53\n",
            "   - -------------------------------------- 18.0/390.3 MB 6.9 MB/s eta 0:00:55\n",
            "   - -------------------------------------- 18.0/390.3 MB 6.8 MB/s eta 0:00:55\n",
            "   - -------------------------------------- 18.1/390.3 MB 6.5 MB/s eta 0:00:57\n",
            "   - -------------------------------------- 18.2/390.3 MB 6.4 MB/s eta 0:00:58\n",
            "   - -------------------------------------- 18.2/390.3 MB 6.2 MB/s eta 0:01:00\n",
            "   - -------------------------------------- 18.4/390.3 MB 6.1 MB/s eta 0:01:02\n",
            "   - -------------------------------------- 18.4/390.3 MB 6.0 MB/s eta 0:01:03\n",
            "   - -------------------------------------- 18.5/390.3 MB 5.8 MB/s eta 0:01:05\n",
            "   - -------------------------------------- 18.6/390.3 MB 5.7 MB/s eta 0:01:05\n",
            "   - -------------------------------------- 18.7/390.3 MB 5.6 MB/s eta 0:01:07\n",
            "   - -------------------------------------- 18.8/390.3 MB 5.5 MB/s eta 0:01:08\n",
            "   - -------------------------------------- 18.9/390.3 MB 5.5 MB/s eta 0:01:09\n",
            "   - -------------------------------------- 19.0/390.3 MB 5.5 MB/s eta 0:01:09\n",
            "   - -------------------------------------- 19.3/390.3 MB 5.5 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 19.7/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 20.1/390.3 MB 5.4 MB/s eta 0:01:10\n",
            "   -- ------------------------------------- 20.6/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 21.1/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 21.7/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 22.3/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 22.8/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 23.4/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 23.9/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 24.5/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 25.0/390.3 MB 5.4 MB/s eta 0:01:09\n",
            "   -- ------------------------------------- 25.6/390.3 MB 5.4 MB/s eta 0:01:08\n",
            "   -- ------------------------------------- 26.1/390.3 MB 5.4 MB/s eta 0:01:08\n",
            "   -- ------------------------------------- 26.7/390.3 MB 5.4 MB/s eta 0:01:08\n",
            "   -- ------------------------------------- 27.2/390.3 MB 5.5 MB/s eta 0:01:07\n",
            "   -- ------------------------------------- 27.7/390.3 MB 6.4 MB/s eta 0:00:58\n",
            "   -- ------------------------------------- 28.3/390.3 MB 7.5 MB/s eta 0:00:49\n",
            "   -- ------------------------------------- 28.8/390.3 MB 9.2 MB/s eta 0:00:40\n",
            "   --- ------------------------------------ 29.3/390.3 MB 11.3 MB/s eta 0:00:32\n",
            "   --- ------------------------------------ 29.8/390.3 MB 11.5 MB/s eta 0:00:32\n",
            "   --- ------------------------------------ 30.4/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 30.9/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 31.5/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 32.0/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 32.6/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 33.1/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 33.7/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 34.2/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 34.7/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 35.3/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 35.9/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 36.4/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 36.9/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 37.5/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 38.0/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   --- ------------------------------------ 38.6/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   ---- ----------------------------------- 39.2/390.3 MB 11.7 MB/s eta 0:00:31\n",
            "   ---- ----------------------------------- 39.7/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 40.3/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 40.8/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 41.4/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 41.9/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 42.4/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 43.0/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 43.6/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 44.1/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 44.7/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 45.2/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 45.8/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 46.3/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 46.9/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 47.4/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ---- ----------------------------------- 48.0/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 48.5/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ----- ---------------------------------- 49.0/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ----- ---------------------------------- 49.6/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ----- ---------------------------------- 50.2/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ----- ---------------------------------- 50.7/390.3 MB 11.7 MB/s eta 0:00:30\n",
            "   ----- ---------------------------------- 51.3/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 51.8/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 52.4/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 52.9/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 53.5/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 54.0/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 54.6/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 55.1/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 55.7/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 56.2/390.3 MB 11.9 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 56.8/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 57.3/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 57.8/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 58.3/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 58.9/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 59.4/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 60.0/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 60.5/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 61.1/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 61.6/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 62.2/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 62.7/390.3 MB 11.7 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 63.2/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 63.8/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 64.3/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 64.9/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 65.5/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 66.0/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 66.5/390.3 MB 11.9 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 67.0/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 67.6/390.3 MB 11.5 MB/s eta 0:00:29\n",
            "   ------ --------------------------------- 68.2/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------- -------------------------------- 68.7/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------- -------------------------------- 69.3/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 69.8/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 70.4/390.3 MB 11.7 MB/s eta 0:00:28\n",
            "   ------- -------------------------------- 70.9/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 71.5/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 72.0/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 72.5/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 73.1/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 73.7/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 74.2/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 74.8/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 75.3/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 75.9/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 76.4/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 77.0/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 77.5/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 78.0/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 78.6/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 79.1/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 79.7/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 80.2/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 80.8/390.3 MB 11.9 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 81.3/390.3 MB 11.9 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 81.9/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 82.4/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 82.9/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 83.5/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 84.0/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 84.6/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 85.1/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 85.7/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 86.2/390.3 MB 11.7 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 86.8/390.3 MB 11.7 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 87.2/390.3 MB 11.5 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 87.2/390.3 MB 11.3 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 87.4/390.3 MB 10.9 MB/s eta 0:00:28\n",
            "   -------- ------------------------------- 87.5/390.3 MB 10.4 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 87.8/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 88.4/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 88.9/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 89.5/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 89.9/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 90.4/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 91.0/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 91.5/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 92.1/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 92.6/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 93.2/390.3 MB 10.2 MB/s eta 0:00:30\n",
            "   --------- ------------------------------ 93.7/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 94.3/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 94.8/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 95.4/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 95.9/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 96.5/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 97.0/390.3 MB 10.2 MB/s eta 0:00:29\n",
            "   --------- ------------------------------ 97.5/390.3 MB 10.9 MB/s eta 0:00:27\n",
            "   ---------- ----------------------------- 98.1/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 98.6/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 99.2/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 99.7/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 100.2/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 100.7/390.3 MB 11.9 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 101.3/390.3 MB 11.9 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 101.8/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 102.4/390.3 MB 11.9 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 102.9/390.3 MB 11.9 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 103.5/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 104.0/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 104.5/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 105.1/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 105.6/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 106.2/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 106.7/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 107.2/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 107.8/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 108.3/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 108.9/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 109.4/390.3 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ---------------------------- 109.9/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 110.5/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 111.0/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 111.6/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 112.1/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 112.7/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 113.2/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 113.8/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 114.3/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 114.9/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 115.4/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 116.0/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 116.4/390.3 MB 11.5 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 117.0/390.3 MB 11.5 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 117.5/390.3 MB 11.5 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 118.1/390.3 MB 11.5 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 118.6/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 119.2/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 119.7/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 120.2/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ------------ -------------------------- 120.8/390.3 MB 11.7 MB/s eta 0:00:24\n",
            "   ------------ -------------------------- 121.4/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 121.8/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 122.4/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 123.0/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 123.5/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 124.1/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 124.6/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 125.2/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 125.8/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 126.3/390.3 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 126.9/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 127.4/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 128.0/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 128.5/390.3 MB 11.9 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 129.1/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------ -------------------------- 129.6/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 130.2/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 130.7/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 131.2/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 131.8/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 132.3/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 132.9/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 133.4/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 134.0/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 134.6/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 135.1/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 135.7/390.3 MB 11.9 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 136.2/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 136.7/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 137.3/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 137.8/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 138.4/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 138.9/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 139.5/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 140.0/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 140.6/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 141.1/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 141.7/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 142.2/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 142.8/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 143.3/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 143.9/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 144.4/390.3 MB 11.7 MB/s eta 0:00:22\n",
            "   -------------- ------------------------ 145.0/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 145.6/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 146.1/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 146.6/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 147.2/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 147.7/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 148.3/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 148.8/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 149.4/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 149.9/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 150.5/390.3 MB 11.9 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 151.1/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 151.6/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 152.1/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 152.7/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 153.2/390.3 MB 11.7 MB/s eta 0:00:21\n",
            "   --------------- ----------------------- 153.8/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 154.3/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 154.9/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 155.4/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 156.0/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 156.5/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 157.1/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 157.6/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 158.1/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 158.6/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 159.2/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 159.7/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 160.3/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 160.8/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 161.4/390.3 MB 11.9 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 161.9/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 162.5/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 163.0/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 163.5/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 164.0/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 164.6/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 165.1/390.3 MB 11.9 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 165.7/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 166.2/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 166.7/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 167.3/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 167.8/390.3 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 168.3/390.3 MB 11.7 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 168.9/390.3 MB 11.7 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 169.4/390.3 MB 11.7 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 169.9/390.3 MB 11.5 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 170.4/390.3 MB 11.5 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 170.9/390.3 MB 11.5 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 171.4/390.3 MB 11.5 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 172.0/390.3 MB 11.5 MB/s eta 0:00:19\n",
            "   ----------------- --------------------- 172.5/390.3 MB 11.5 MB/s eta 0:00:19\n",
            "   ----------------- --------------------- 172.7/390.3 MB 11.5 MB/s eta 0:00:19\n",
            "   ----------------- --------------------- 173.3/390.3 MB 11.1 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 173.8/390.3 MB 11.1 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 174.4/390.3 MB 11.1 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 174.8/390.3 MB 11.1 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 175.3/390.3 MB 10.9 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 175.8/390.3 MB 10.9 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 176.1/390.3 MB 10.9 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 176.6/390.3 MB 10.7 MB/s eta 0:00:20\n",
            "   ----------------- --------------------- 177.0/390.3 MB 10.6 MB/s eta 0:00:21\n",
            "   ----------------- --------------------- 177.0/390.3 MB 10.2 MB/s eta 0:00:21\n",
            "   ----------------- --------------------- 177.0/390.3 MB 10.1 MB/s eta 0:00:22\n",
            "   ------------------ --------------------- 177.3/390.3 MB 9.5 MB/s eta 0:00:23\n",
            "   ------------------ --------------------- 177.4/390.3 MB 9.2 MB/s eta 0:00:24\n",
            "   ------------------ --------------------- 177.5/390.3 MB 8.8 MB/s eta 0:00:25\n",
            "   ------------------ --------------------- 177.5/390.3 MB 8.7 MB/s eta 0:00:25\n",
            "   ------------------ --------------------- 177.5/390.3 MB 8.3 MB/s eta 0:00:26\n",
            "   ------------------ --------------------- 177.6/390.3 MB 8.2 MB/s eta 0:00:26\n",
            "   ------------------ --------------------- 177.6/390.3 MB 7.7 MB/s eta 0:00:28\n",
            "   ------------------ --------------------- 177.7/390.3 MB 7.4 MB/s eta 0:00:29\n",
            "   ------------------ --------------------- 177.8/390.3 MB 7.4 MB/s eta 0:00:29\n",
            "   ------------------ --------------------- 177.9/390.3 MB 7.1 MB/s eta 0:00:30\n",
            "   ------------------ --------------------- 178.0/390.3 MB 7.0 MB/s eta 0:00:31\n",
            "   ------------------ --------------------- 178.2/390.3 MB 6.7 MB/s eta 0:00:32\n",
            "   ------------------ --------------------- 178.3/390.3 MB 6.7 MB/s eta 0:00:32\n",
            "   ------------------ --------------------- 178.4/390.3 MB 6.5 MB/s eta 0:00:33\n",
            "   ------------------ --------------------- 178.5/390.3 MB 6.4 MB/s eta 0:00:34\n",
            "   ------------------ --------------------- 178.6/390.3 MB 6.2 MB/s eta 0:00:34\n",
            "   ------------------ --------------------- 178.7/390.3 MB 6.0 MB/s eta 0:00:36\n",
            "   ------------------ --------------------- 178.8/390.3 MB 5.9 MB/s eta 0:00:36\n",
            "   ------------------ --------------------- 178.9/390.3 MB 5.8 MB/s eta 0:00:37\n",
            "   ------------------ --------------------- 179.0/390.3 MB 5.7 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 179.3/390.3 MB 5.5 MB/s eta 0:00:39\n",
            "   ------------------ --------------------- 179.8/390.3 MB 5.6 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 180.3/390.3 MB 5.6 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 180.8/390.3 MB 5.5 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 181.3/390.3 MB 5.6 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 181.8/390.3 MB 5.6 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 182.3/390.3 MB 5.5 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 182.9/390.3 MB 5.6 MB/s eta 0:00:38\n",
            "   ------------------ --------------------- 183.4/390.3 MB 5.6 MB/s eta 0:00:37\n",
            "   ------------------ --------------------- 184.0/390.3 MB 5.6 MB/s eta 0:00:37\n",
            "   ------------------ --------------------- 184.5/390.3 MB 5.6 MB/s eta 0:00:37\n",
            "   ------------------ --------------------- 185.1/390.3 MB 5.7 MB/s eta 0:00:37\n",
            "   ------------------- -------------------- 185.6/390.3 MB 5.7 MB/s eta 0:00:36\n",
            "   ------------------- -------------------- 186.2/390.3 MB 5.7 MB/s eta 0:00:36\n",
            "   ------------------- -------------------- 186.7/390.3 MB 5.8 MB/s eta 0:00:36\n",
            "   ------------------- -------------------- 187.3/390.3 MB 6.2 MB/s eta 0:00:33\n",
            "   ------------------- -------------------- 187.8/390.3 MB 7.3 MB/s eta 0:00:28\n",
            "   ------------------- -------------------- 188.4/390.3 MB 8.4 MB/s eta 0:00:25\n",
            "   ------------------ -------------------- 188.9/390.3 MB 10.1 MB/s eta 0:00:20\n",
            "   ------------------ -------------------- 189.5/390.3 MB 11.5 MB/s eta 0:00:18\n",
            "   ------------------ -------------------- 190.1/390.3 MB 11.7 MB/s eta 0:00:18\n",
            "   ------------------- ------------------- 190.6/390.3 MB 11.7 MB/s eta 0:00:18\n",
            "   ------------------- ------------------- 191.2/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 191.7/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 192.3/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 192.8/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 193.4/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 193.8/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 194.3/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 194.9/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 195.4/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 196.0/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 196.5/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 197.0/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 197.6/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 198.1/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 198.7/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 199.2/390.3 MB 11.9 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 199.8/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 200.3/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 200.9/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 201.4/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 201.9/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 202.5/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 203.1/390.3 MB 11.7 MB/s eta 0:00:17\n",
            "   -------------------- ------------------ 203.6/390.3 MB 11.9 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 204.2/390.3 MB 11.9 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 204.7/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 205.3/390.3 MB 11.9 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 205.8/390.3 MB 11.9 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 206.3/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 206.8/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 207.4/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 207.9/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 208.5/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 209.0/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 209.6/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 210.1/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 210.7/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 211.2/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 211.8/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 212.3/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 212.8/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 213.4/390.3 MB 11.7 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 213.9/390.3 MB 11.5 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 214.5/390.3 MB 11.5 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 215.0/390.3 MB 11.5 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 215.5/390.3 MB 11.5 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 216.1/390.3 MB 11.5 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 216.6/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 217.1/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 217.7/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 218.2/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 218.8/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 219.3/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 219.9/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 220.4/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 220.9/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 221.5/390.3 MB 11.9 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 222.0/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 222.6/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 223.1/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 223.6/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 224.2/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 224.7/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 225.3/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 225.9/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 226.4/390.3 MB 11.7 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 227.0/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 227.5/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 228.1/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 228.6/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 229.1/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 229.7/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 230.2/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 230.8/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 231.3/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 231.9/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 232.4/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 233.0/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 233.6/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 234.1/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 234.6/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 235.2/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 235.7/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 236.3/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 236.8/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 237.4/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 237.9/390.3 MB 11.7 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 238.5/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ----------------------- --------------- 239.1/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ----------------------- --------------- 239.6/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 240.2/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 240.8/390.3 MB 11.9 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 241.3/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 241.8/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 242.4/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 242.9/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 243.5/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 244.0/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 244.6/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 245.1/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 245.7/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 246.2/390.3 MB 11.7 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 246.7/390.3 MB 11.9 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 247.2/390.3 MB 11.9 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 247.8/390.3 MB 11.9 MB/s eta 0:00:12\n",
            "   ------------------------ -------------- 248.3/390.3 MB 11.9 MB/s eta 0:00:12\n",
            "   ------------------------ -------------- 248.9/390.3 MB 11.9 MB/s eta 0:00:12\n",
            "   ------------------------ -------------- 249.4/390.3 MB 11.9 MB/s eta 0:00:12\n",
            "   ------------------------ -------------- 250.0/390.3 MB 11.9 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 250.5/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 251.1/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 251.6/390.3 MB 11.9 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 252.0/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 252.6/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 253.1/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 253.7/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 254.2/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 254.8/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 255.3/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 255.9/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 256.4/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 256.9/390.3 MB 11.5 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 257.5/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 258.1/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 258.7/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 259.1/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 259.7/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   -------------------------- ------------ 260.2/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   -------------------------- ------------ 260.8/390.3 MB 11.5 MB/s eta 0:00:12\n",
            "   -------------------------- ------------ 261.3/390.3 MB 11.7 MB/s eta 0:00:12\n",
            "   -------------------------- ------------ 261.9/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 262.4/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 263.0/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 263.5/390.3 MB 11.9 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 264.0/390.3 MB 11.9 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 264.6/390.3 MB 11.9 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 265.1/390.3 MB 11.9 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 265.7/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 266.2/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 266.8/390.3 MB 11.9 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 267.3/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 267.9/390.3 MB 11.9 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 268.3/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 268.9/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 269.4/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 270.0/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 270.5/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 271.1/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 271.6/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 272.2/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 272.8/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 273.3/390.3 MB 11.7 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 273.9/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 274.4/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 274.9/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 275.5/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 276.0/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 276.6/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 277.1/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 277.7/390.3 MB 11.9 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 278.2/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 278.8/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 279.3/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 279.8/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 280.4/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 280.9/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 281.5/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 282.0/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 282.6/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 283.1/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 283.7/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 284.2/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 284.8/390.3 MB 11.7 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 285.3/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 285.9/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 286.4/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 286.9/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 287.4/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 288.0/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 288.5/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 289.1/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 289.5/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 290.1/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 290.6/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 291.2/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 291.7/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 292.3/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 292.8/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 293.4/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 293.9/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 294.5/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 295.0/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 295.5/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 296.1/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 296.6/390.3 MB 11.7 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 297.2/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 297.7/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 298.2/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 298.8/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 299.4/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 299.9/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 300.5/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 301.0/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 301.6/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 302.1/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 302.7/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 303.2/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 303.7/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 304.3/390.3 MB 11.9 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 304.8/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 305.3/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 305.9/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 306.4/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 307.0/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 307.5/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 308.1/390.3 MB 11.7 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 308.6/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------ -------- 309.2/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------ -------- 309.7/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 310.3/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 310.8/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 311.4/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 311.9/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 312.5/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 313.0/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 313.5/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 314.1/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 314.7/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 315.2/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 315.8/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 316.3/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 316.8/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 317.4/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 317.9/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 318.5/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 319.0/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 319.6/390.3 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 320.1/390.3 MB 11.9 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 320.6/390.3 MB 11.9 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 321.2/390.3 MB 11.9 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 321.7/390.3 MB 11.9 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 322.2/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 322.8/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 323.3/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 323.9/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 324.4/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 325.0/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 325.5/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 326.0/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 326.5/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 327.1/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 327.7/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 328.2/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 328.8/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 329.3/390.3 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 329.5/390.3 MB 11.3 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 329.7/390.3 MB 10.9 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 329.8/390.3 MB 10.6 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 329.9/390.3 MB 10.1 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 330.0/390.3 MB 9.8 MB/s eta 0:00:07\n",
            "   --------------------------------- ------ 330.1/390.3 MB 9.4 MB/s eta 0:00:07\n",
            "   --------------------------------- ------ 330.3/390.3 MB 9.1 MB/s eta 0:00:07\n",
            "   --------------------------------- ------ 330.3/390.3 MB 9.0 MB/s eta 0:00:07\n",
            "   --------------------------------- ------ 330.4/390.3 MB 8.6 MB/s eta 0:00:07\n",
            "   --------------------------------- ------ 330.5/390.3 MB 8.3 MB/s eta 0:00:08\n",
            "   --------------------------------- ------ 330.7/390.3 MB 8.0 MB/s eta 0:00:08\n",
            "   --------------------------------- ------ 330.8/390.3 MB 7.8 MB/s eta 0:00:08\n",
            "   --------------------------------- ------ 330.9/390.3 MB 7.6 MB/s eta 0:00:08\n",
            "   --------------------------------- ------ 331.0/390.3 MB 7.4 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 331.0/390.3 MB 7.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 331.1/390.3 MB 7.0 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 331.2/390.3 MB 6.8 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 331.2/390.3 MB 6.6 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 331.3/390.3 MB 6.4 MB/s eta 0:00:10\n",
            "   --------------------------------- ------ 331.3/390.3 MB 6.3 MB/s eta 0:00:10\n",
            "   --------------------------------- ------ 331.4/390.3 MB 6.1 MB/s eta 0:00:10\n",
            "   --------------------------------- ------ 331.5/390.3 MB 6.0 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 331.7/390.3 MB 5.8 MB/s eta 0:00:11\n",
            "   ---------------------------------- ----- 332.1/390.3 MB 5.8 MB/s eta 0:00:11\n",
            "   ---------------------------------- ----- 332.5/390.3 MB 5.8 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 333.0/390.3 MB 5.8 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 333.6/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 334.1/390.3 MB 5.8 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 334.6/390.3 MB 5.8 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 335.1/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 335.6/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 336.1/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 336.7/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 337.2/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 337.8/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 338.3/390.3 MB 5.7 MB/s eta 0:00:10\n",
            "   ---------------------------------- ----- 338.9/390.3 MB 5.7 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 339.4/390.3 MB 5.7 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 340.0/390.3 MB 6.1 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 340.5/390.3 MB 6.8 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 341.0/390.3 MB 7.7 MB/s eta 0:00:07\n",
            "   ---------------------------------- ---- 341.6/390.3 MB 10.1 MB/s eta 0:00:05\n",
            "   ---------------------------------- ---- 342.1/390.3 MB 11.3 MB/s eta 0:00:05\n",
            "   ---------------------------------- ---- 342.6/390.3 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------------- ---- 343.1/390.3 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------------- ---- 343.7/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 344.2/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 344.8/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 345.3/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 345.8/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 346.4/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 346.9/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 347.5/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 348.0/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 348.5/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 349.1/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 349.6/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 350.2/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 350.8/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 351.3/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 351.9/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 352.4/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 353.0/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 353.5/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 354.0/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 354.6/390.3 MB 11.7 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 355.2/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 355.7/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 356.3/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 356.9/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 357.4/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 357.9/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 358.4/390.3 MB 11.7 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 358.9/390.3 MB 11.7 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 359.5/390.3 MB 11.7 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 360.0/390.3 MB 11.7 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 360.6/390.3 MB 11.7 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 361.1/390.3 MB 11.7 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 361.7/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 362.2/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 362.8/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 363.3/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 363.8/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 364.4/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 364.9/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 365.5/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 366.0/390.3 MB 11.9 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 366.6/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 367.1/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 367.7/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 368.2/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 368.8/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 369.3/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 369.9/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 370.4/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 371.0/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 371.5/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 372.1/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 372.6/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 373.2/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 373.7/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 374.2/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 374.8/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 375.3/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 375.9/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 376.5/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 377.0/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 377.6/390.3 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 378.1/390.3 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 378.7/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 379.2/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 379.8/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  380.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  380.8/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  381.4/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  381.8/390.3 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  382.4/390.3 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  383.0/390.3 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  383.5/390.3 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  384.1/390.3 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  384.6/390.3 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  385.1/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  385.6/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  386.2/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  386.8/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  387.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  387.9/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  388.4/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  389.0/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  389.5/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.1/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  390.3/390.3 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 390.3/390.3 MB 7.5 MB/s eta 0:00:00\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
            "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.4/4.3 MB 13.5 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 1.0/4.3 MB 12.1 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 1.5/4.3 MB 11.9 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 2.0/4.3 MB 11.5 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 2.5/4.3 MB 11.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 3.0/4.3 MB 11.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 3.6/4.3 MB 11.5 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 4.1/4.3 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.3/4.3 MB 11.4 MB/s eta 0:00:00\n",
            "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.6/26.4 MB 12.6 MB/s eta 0:00:03\n",
            "   - -------------------------------------- 1.1/26.4 MB 12.2 MB/s eta 0:00:03\n",
            "   -- ------------------------------------- 1.7/26.4 MB 11.9 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 2.2/26.4 MB 11.9 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 2.8/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 3.3/26.4 MB 11.8 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 3.9/26.4 MB 11.8 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 4.4/26.4 MB 11.8 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 4.9/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 5.5/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 6.0/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 6.6/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 7.1/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 7.6/26.4 MB 11.6 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 8.2/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 8.7/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 9.3/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 9.8/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 10.4/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 10.9/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 11.5/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 12.0/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 12.6/26.4 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 12.9/26.4 MB 11.5 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 13.0/26.4 MB 11.1 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 13.5/26.4 MB 10.9 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 14.0/26.4 MB 10.9 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 14.6/26.4 MB 10.9 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 15.1/26.4 MB 10.9 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 15.7/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 16.2/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 16.8/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 17.3/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 17.8/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 18.4/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 18.9/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 19.5/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 20.0/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 20.6/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 21.0/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 21.5/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 22.1/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 22.6/26.4 MB 10.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 23.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 23.7/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 24.3/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 24.8/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 25.4/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.9/26.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.4/26.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 26.4/26.4 MB 11.3 MB/s eta 0:00:00\n",
            "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
            "   ---------------------------------------- 0.0/127.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 127.5/127.5 kB 3.7 MB/s eta 0:00:00\n",
            "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 71.9/71.9 kB 4.1 MB/s eta 0:00:00\n",
            "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.5/5.5 MB 9.8 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 1.0/5.5 MB 10.7 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 1.6/5.5 MB 11.0 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 2.1/5.5 MB 12.1 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 2.7/5.5 MB 11.2 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 3.2/5.5 MB 12.0 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 3.7/5.5 MB 11.9 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 4.3/5.5 MB 11.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 4.8/5.5 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  5.4/5.5 MB 11.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 5.5/5.5 MB 11.3 MB/s eta 0:00:00\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: libclang, flatbuffers, tensorboard-data-server, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow-intel, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.1.24 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 libclang-18.1.1 ml-dtypes-0.4.1 opt-einsum-3.4.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp9BPRd1tMIo",
        "outputId": "d60def97-3839-4c8f-caed-24b63b0164ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDs-MYtYH8sL"
      },
      "source": [
        "## 3.4. Attention Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT"
      },
      "source": [
        "어텐션 마스크는 어떤 토큰이 실제 단어인지, 어떤 토큰이 패딩인지를 명확하게 보여줍니다.\n",
        "\n",
        "BERT 어휘는 ID 0을 사용하지 않으므로 토큰 ID가 0이면 패딩이고 그렇지 않으면 실제 토큰입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDoC24LeEv3N"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_"
      },
      "source": [
        "## 3.5. Training & Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0ao7p8rb06"
      },
      "source": [
        "train/test를 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR"
      },
      "source": [
        "## 3.6. Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1uXczp-Je4"
      },
      "source": [
        "우리 모델은 numpy.ndarrays 대신 PyTorch 텐서를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "outputs": [],
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD9i6Z2pG-sN"
      },
      "source": [
        "또한 토치 DataLoader 클래스를 사용하여 데이터세트에 대한 반복자를 생성합니다. 이는 for 루프와 달리 반복자를 사용하면 전체 데이터세트를 메모리에 로드할 필요가 없기 때문에 훈련 중에 메모리를 절약하는 데 도움이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# 4. Train Our Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "## 4.1. BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sjzRT1V0zwm"
      },
      "source": [
        "이 작업을 위해 먼저 사전 훈련된 BERT 모델을 수정하여 분류를 위한 출력을 제공한 다음 전체 모델이 엔드투엔드에 적합할 때까지 데이터 세트에서 모델을 계속 훈련하려고 합니다.\n",
        "\n",
        "현재 미세 조정을 위해 Huggingface에서 제공되는 클래스 목록은 다음과 같습니다.\n",
        "* BertModel\n",
        "* BertForPreTraining\n",
        "* BertForMaskedLM\n",
        "* BertForNextSentence예측\n",
        "* **BertForSequenceClassification** -> 우리가 사용할 것입니다.\n",
        "* BertForTokenClassification\n",
        "* BertForQuestionAnswering\n",
        "\n",
        "이에 대한 문서는 [여기](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)에서 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXYitPoE-cjH"
      },
      "source": [
        "우리는 [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)을 사용할 것입니다. 이것은 문장 분류기로 사용할 분류를 위해 상단에 단일 선형 레이어가 추가된 일반 BERT 모델입니다. 입력 데이터를 제공하면 사전 훈련된 전체 BERT 모델과 훈련되지 않은 추가 분류 계층이 특정 작업에 대해 훈련됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnQW9E-bBCRt"
      },
      "source": [
        "`from_pretrained` 에 대한 문서는 [여기](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)에 있습니다.\n",
        "추가적인 parameter는 [여기](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)에 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFsCTp_mporB",
        "outputId": "67f2072a-dc75-45cf-f790-5f6e0b74597e",
        "colab": {
          "referenced_widgets": [
            "260cc06f97ec47719bb0c1b5e631b6b2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "260cc06f97ec47719bb0c1b5e631b6b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "outputId": "bf92aef4-7ce0-4923-d744-8c38ba8217c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## 4.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "이제 모델을 로드했으므로 저장된 모델 내에서 훈련 하이퍼파라미터를 가져와야 합니다.\n",
        "\n",
        "미세 조정을 위해 저자는 다음 값 중에서 선택할 것을 권장합니다.\n",
        "- batch size: 16, 32(DataLoader를 생성할 때 32를 선택했습니다).\n",
        "- learning rate(Adam): 5e-5, 3e-5, 2e-5(여기에서는 2e-5를 사용하겠습니다).\n",
        "- epochs: 2, 3, 4(여기에서는 4를 사용합니다).\n",
        "\n",
        "엡실론 매개변수 `eps = 1e-8`은 \"구현 시 0으로 나누는 것을 방지하기 위한 매우 작은 숫자\"입니다([여기](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "'run_glue.py' [여기](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)에서 AdamW 최적화 프로그램 생성을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLs72DuMODJO",
        "outputId": "7d9f4469-38ed-4ce5-b808-24b90d0fa0ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## 4.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXZhFb4LnV5"
      },
      "source": [
        "다음은 훈련 루프입니다. 많은 일이 진행되고 있지만 기본적으로 루프의 각 패스에는 트라이닝 단계와 검증 단계가 있습니다. 각 패스에서 우리는 다음을 수행해야 합니다.\n",
        "\n",
        "훈련 루프:\n",
        "- 데이터 입력 및 라벨 압축 풀기\n",
        "- 가속을 위해 GPU에 데이터 로드\n",
        "- 이전 단계에서 계산된 그래디언트를 지웁니다.\n",
        "     - pytorch에서는 명시적으로 지우지 않는 한 기본적으로 그래디언트가 누적됩니다(RNN과 같은 작업에 유용함).\n",
        "- 순방향 패스(네트워크를 통해 입력 데이터 공급)\n",
        "- 역방향 전달(역전파)\n",
        "- 네트워크에 Optimizer.step()을 사용하여 매개변수를 업데이트하도록 지시합니다.\n",
        "- 진행상황 모니터링을 위한 변수 추적\n",
        "\n",
        "평가 루프:\n",
        "- 데이터 입력 및 라벨 압축 풀기\n",
        "- 가속을 위해 GPU에 데이터 로드\n",
        "- 순방향 패스(네트워크를 통해 입력 데이터 공급)\n",
        "- 검증 데이터의 손실을 계산하고 진행 상황을 모니터링하기 위한 변수를 추적합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "outputId": "fbcee178-4a3e-44ee-ca70-61de0cb8ee4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:29.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:22.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:28.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:22.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:28.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:22.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:28.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68xreA9JAmG5",
        "outputId": "fdb5ddb0-fc3f-4ba0-c811-34a082d75c88"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# 5. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "test 용 데이터세트를 로드하고 [Matthew의 상관 계수](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)를 사용하여 예측을 평가합니다.\n",
        "\n",
        "이 지표를 사용하면 +1이 최고 점수이고 -1이 최저 점수입니다. 이런 식으로 우리는 이 특정 작업에 대한 최신 모델에 비해 우리가 얼마나 잘 수행하는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### 5.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "outputId": "0e5a9a92-97d2-4f92-c90e-76738f3c08dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"C:/Users/User/Desktop/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "outputId": "1f382568-642c-488d-8eed-c87f2ea7da37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWcy0X1hirdx",
        "outputId": "21c5f6fd-dfd3-4b2f-eb43-11601fe630f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "outputId": "2be15f07-8201-4c4c-a35f-3029e1bf56e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xytAr_C48wnu",
        "outputId": "fafdfbde-7f7f-45d3-9f86-1b7db88fab70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.049286405809014416,\n",
              " -0.21684543705982773,\n",
              " 0.4040950971038548,\n",
              " 0.23372319715296222,\n",
              " 0.3567530340063379,\n",
              " 0.7410010097502685,\n",
              " 0.4547940268270977,\n",
              " 0.0,\n",
              " 0.9165151389911681,\n",
              " 0.6386392673039035,\n",
              " 0.8459051693633014,\n",
              " 0.647150228929434,\n",
              " 0.8749672939989046,\n",
              " 0.7141684885491869,\n",
              " 0.1794871794871795,\n",
              " 0.717973623597536,\n",
              " 0.0]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matthews_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "outputId": "9b4c94a9-1516-439e-cadd-6e4e87f1b009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MCC: 0.524\n"
          ]
        }
      ],
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}