{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["7lMBEMIXeOQe","lGaWC_GlZ8kM","Pbyq6TXldnDJ","Qo3hh_Y2akqu","2Y3Nm8I2bFqX","lrCQ_M0-0Omz","pCRx0d280HBA"],"authorship_tag":"ABX9TyOOOJe/ctmTGXF/Vgt1Qrdi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### 필요 라이브러리"],"metadata":{"id":"aG5uadYZcpt1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZJ6UV7Ja1zq"},"outputs":[],"source":["# 데이터 처리\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import json\n","import itertools\n","import cv2\n","from torch.utils.data import DataLoader\n","\n","# ViT\n","from timm import create_model, list_models\n","from types import SimpleNamespace\n","\n","# GPT\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\n","\n","# 데이터 증강\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2 # 데이터 -> PyTorch 텐서로 변환\n","from PIL import Image # 이미지 처리\n","from pathlib import Path # 파일 경로 관리\n","from sklearn.model_selection import train_test_split\n","from torch.cuda.amp import GradScaler, autocast\n","from tqdm.auto import tqdm # 진행 상태 표시\n","import gc\n","\n","# 최종 예측\n","import subprocess\n","import matplotlib.pyplot as plt\n","from IPython.display import display, Video"]},{"cell_type":"markdown","source":["### Fine-tuning the Model"],"metadata":{"id":"QdqR4yYZcsjO"}},{"cell_type":"markdown","source":["#### Loading the Data"],"metadata":{"id":"iLqZehLdcvUO"}},{"cell_type":"code","source":["tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.pad_token"],"metadata":{"id":"sW1usTMecxVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 데이터 불러오기\n","base_path = '/content/drive/MyDrive/DATA_최종'\n","images_path = os.path.join(base_path, 'Images')\n","labels_path = os.path.join(base_path, 'Labels')"],"metadata":{"id":"Ut4GmCqtc0Gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 영상 프레임 가져오기\n","\n","# 행동 폴더 이름 -> action_classes\n","action_classes = os.listdir(images_path)\n","print(f\"고냥이의 행동 {len(action_classes)}개 있어요 ฅ^._.^ฅ\")\n","\n","# 각 폴더로 들어가서 - 비디오 이름들 가져오고 - 그 비디오 폴더 안으로 다시 들어가서 - 비디오 프레임들 가져오기 (200개 정도만?)\n","frames = {}\n","videos = {}\n","\n","for action in action_classes:\n","    videos[action] = [] # 초기화\n","    frames[action] = [] # 초기화\n","\n","    images_path_behav = os.path.join(images_path, action) # 각 폴더 경로\n","    video_names = os.listdir(images_path_behav) # 비디오 이름들 가져오기\n","\n","    count = 0\n","    for video in video_names:\n","        if count >= 100:\n","          break\n","        frames_path = os.path.join(images_path_behav, video) # 각 영상 경로\n","        frame_names = os.listdir(frames_path) # 각 영상의 프레임 이름들\n","\n","        if not frame_names:\n","          print(f\"경고: {frames_path} 폴더가 비어 있습니다. 건너뜁니다.\")\n","          continue\n","\n","        one_video_frames = [os.path.join(frames_path, fname) for fname in frame_names]\n","        frames[action].append(one_video_frames)\n","        videos[action].append(video)\n","        count += 1"],"metadata":{"id":"sxa1tUQdc28p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 영상 텍스트 가져와서 labels에 저장\n","labels = []\n","for action in videos.keys(): # 각 행동별로\n","    for video in videos[action]: # 각 비디오별로\n","        label_path = os.path.join(labels_path, action, video) + '.json' # json 파일 경로 설정\n","        try:\n","          with open(label_path, 'r') as f:\n","              json_file = json.load(f)\n","              labels.append(\"고양이가 \" + json_file['metadata']['owner']['situation'] + \" \" + json_file['metadata']['action']) # situation, action 불러오기\n","        except Exception as e:\n","          print(f\"{label_path} 에서 오류 발생: {e}\")\n"],"metadata":{"id":"GJTRxtW6c72E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deepl"],"metadata":{"id":"5cjEoMSjc-GU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import deepl\n","\n","AUTH_KEY = \"##########\"\n","translator = deepl.Translator(AUTH_KEY)\n","\n","batch_size = 10\n","translated_labels = []\n","\n","for i in range(0, len(labels), batch_size):\n","    batch = labels[i:i+batch_size]\n","    translated_results = translator.translate_text(batch, source_lang=\"KO\", target_lang=\"EN-US\")\n","    translated_labels.extend([result.text for result in translated_results])"],"metadata":{"id":"1LePa6dDc_U2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_videos = list(itertools.chain(*videos.values()))\n","all_frames = list(itertools.chain(*frames.values()))"],"metadata":{"id":"ATaDFDPpdKm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 데이터프레임으로\n","kitties = pd.DataFrame({'video_names' : all_videos, 'videos' : all_frames, 'caption' : translated_labels})\n","kitties.head(2)"],"metadata":{"id":"Zwa681ApdLuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 잘 로드되었는지 확인\n","image_path = kitties['videos'][5][13]\n","image = cv2.imread(image_path)\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","plt.imshow(image)"],"metadata":{"id":"yPt63kQqdNTr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dataset, DataLoader"],"metadata":{"id":"7lMBEMIXeOQe"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, df, tokenizer, transform):\n","        self.df = df\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        sample = self.df.iloc[idx, : ]\n","        frame_paths = sample['videos']\n","        caption = sample['caption']\n","\n","        # 이미지 불러오기\n","        frames = []\n","        for frame_path in frame_paths:\n","            image = cv2.imread(frame_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = np.array(image, dtype = np.uint8)\n","            image = self.transform(image = image)[\"image\"] # 바뀐 애들 중에서 image만 떼어오기\n","            frames.append(image)\n","\n","        frames_tensor = torch.stack([torch.as_tensor(frame, dtype = torch.float32).clone().detach() for frame in frames])\n","\n","        # 텍스트 토큰화\n","        caption = f\"{caption}<|endoftext|>\"\n","        input_ids = self.tokenizer(\n","            caption,\n","            truncation=True,\n","            return_tensors = \"pt\")['input_ids'].squeeze(0) # squeeze by gpt\n","        labels = input_ids.clone()\n","        labels[ :-1] = input_ids[1: ]\n","        return frames_tensor, input_ids, labels"],"metadata":{"id":"8ILGWwv4dQIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_kitties, val_kitties = train_test_split(kitties, test_size=0.1)"],"metadata":{"id":"Nt8roYZQdWdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_kitties = Dataset(train_kitties, tokenizer, transform)\n","val_kitties = Dataset(val_kitties, tokenizer, transform)"],"metadata":{"id":"LOuz2PIUdXFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 영상들의 프레임 개수 리스트 - 얼마로 패딩할지 정하기 위함 (평균 프레임 개수로 정했습니당)\n","frames_num = []\n","for i in range(len(kitties)):\n","    frames_num.append(len(kitties.loc[i, 'videos']))\n","\n","video_padding = np.mean(frames_num)"],"metadata":{"id":"zoq1twnldXvm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    frames, input_ids, labels = zip(*batch)\n","\n","    # 이미지 패딩\n","    padding_size = int(video_padding)\n","    padded_frames = []\n","\n","    for video in frames:\n","        current_length = video.shape[0]\n","\n","        if current_length > padding_size: # 패딩보다 길다면\n","            video = video[ :padding_size] # 잘라내고\n","        else:\n","            pad_size = padding_size - current_length # 짧으면 부족한 만큼 채우기\n","            pad_tensor = torch.zeros((pad_size, *video.shape[1: ]))\n","            video = torch.cat([video, pad_tensor], dim = 0)\n","\n","        padded_frames.append(video)\n","\n","    padded_frames_together = torch.stack(padded_frames)\n","\n","    # 텍스트 패딩\n","    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first = True)\n","    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first = True, padding_value = -100) # 나중에 loss 계산할 때 무시하라고 -100으로 처리\n","\n","    return padded_frames_together, input_ids, labels"],"metadata":{"id":"BGXKvAsWdYgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### DataLoader 설정\n","train_dataloader = DataLoader(\n","    train_kitties,\n","    batch_size = 4,\n","    shuffle = True,\n","    num_workers = 1,\n","    collate_fn = collate_fn)\n","\n","val_dataloader = DataLoader(\n","    val_kitties,\n","    batch_size = 4,\n","    shuffle = False,\n","    num_workers = 1,\n","    collate_fn = collate_fn)"],"metadata":{"id":"_pCR3OcvdZgI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Structures"],"metadata":{"id":"w3W5-sFSdhwf"}},{"cell_type":"markdown","metadata":{"id":"lGaWC_GlZ8kM"},"source":["#### Model Structure - GPT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeOFHPKlZ_4C"},"outputs":[],"source":["class GPT2Attention(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.n_heads = config.num_heads\n","        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n","        self.head_size = self.embed_dim // self.n_heads\n","        self.seq_len = config.seq_len\n","\n","        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n","        self.scale = self.head_size ** -0.5\n","\n","        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n","\n","        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n","\n","        self.attn_dropout = nn.Dropout(config.attention_dropout)\n","        self.resid_dropout = nn.Dropout(config.residual_dropout)\n","\n","\n","    def forward(self, x):\n","        b,t,c = x.shape\n","        # q,k,v shape individually: batch_size x seq_len x embed_dim\n","        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n","        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n","        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n","        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n","        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n","\n","        qk_t = (q@k.transpose(-2,-1)) * self.scale\n","        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n","        qk_t = F.softmax(qk_t,dim=-1)\n","        weights = self.attn_dropout(qk_t)\n","\n","        attention = weights @ v # batch x n_heads x t x head_size\n","        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n","\n","        out = self.c_proj(attention)\n","        out = self.resid_dropout(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfriaGhIaEn9"},"outputs":[],"source":["class GPT2CrossAttention(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.n_heads = config.num_heads\n","        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n","        self.head_size = self.embed_dim // self.n_heads\n","        self.seq_len = config.seq_len\n","\n","        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n","        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n","        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n","        self.scale = self.head_size ** -0.5\n","\n","        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n","\n","        self.attn_dropout = nn.Dropout(config.attention_dropout)\n","        self.resid_dropout = nn.Dropout(config.residual_dropout)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","\n","    def forward(self, q,k,v):\n","        b,t,c = q.shape\n","\n","        q = self.q(q)\n","        k = self.k(k)\n","        v = self.v(v)\n","\n","        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n","        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n","        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n","\n","        qk_t = (q@k.transpose(-2,-1)) * self.scale\n","        qk_t = F.softmax(qk_t,dim=-1)\n","        weights = self.attn_dropout(qk_t)\n","\n","        attention = weights @ v # batch x n_heads x t x head_size\n","        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n","\n","        out = self.c_proj(attention)\n","        out = self.resid_dropout(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvg_1pMqaKse"},"outputs":[],"source":["class GPT2MLP(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.mlp_ratio = config.mlp_ratio\n","        self.mlp_dropout = config.mlp_dropout\n","\n","        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n","        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n","        self.act = nn.GELU()\n","        self.dropout = nn.Dropout(self.mlp_dropout)\n","\n","    def forward(self,x):\n","        x = self.c_fc(x)\n","        x = self.act(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ea6HEeauaLyW"},"outputs":[],"source":["class GPT2Block(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.ln_1 = nn.LayerNorm(self.embed_dim)\n","        self.attn = GPT2Attention(config)\n","        self.ln_2 = nn.LayerNorm(self.embed_dim)\n","        self.mlp = GPT2MLP(config)\n","        self.ln_3 = nn.LayerNorm(self.embed_dim)\n","        self.cross_attn = GPT2CrossAttention(config)\n","\n","    def forward(self,x,enc_out):\n","        x = x+self.attn(self.ln_1(x))\n","        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out)\n","        x = x+self.mlp(self.ln_3(x))\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"HwKAjm2zaNCx"},"source":["#### Model Structure - ViT & GPT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQUreTRAaOaV"},"outputs":[],"source":["class VisionGPT2Model(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","\n","        self.config = config\n","\n","        ##### 이거로 ViT 생성 ㅇmㅇ..\n","        vit = create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n","        self.patch_embed = vit.patch_embed # 입력 이미지마다 패치로 분할, 각 패치별로 임베딩 벡터\n","        num_patches = self.patch_embed.num_patches\n","        self.cls_token = vit.cls_token\n","        embed_len = num_patches + vit.num_prefix_tokens\n","        self.pos_embed = vit.pos_embed\n","        self.pos_drop = nn.Dropout(p=0.)\n","\n","        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n","            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n","            drop = nn.Dropout(config.emb_dropout),\n","            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n","            ln_f = nn.LayerNorm(config.embed_dim)\n","        ))\n","        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","    def _pos_embed(self,x):\n","        pos_embed = self.pos_embed\n","        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n","        x = x + pos_embed\n","        return self.pos_drop(x)\n","\n","    def pretrained_layers_trainable(self,trainable=False):\n","        layers = [\n","            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n","            self.transformer.wte, self.transformer.wpe,\n","            self.transformer.ln_f, self.lm_head\n","        ]\n","        gpt_layers = [[\n","            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n","            self.transformer.h[i].attn,self.transformer.h[i].mlp\n","        ] for i in range(self.config.depth)]\n","        for l in gpt_layers:\n","            layers.extend(l)\n","\n","        for layer in layers:\n","            if not isinstance(layer,nn.Parameter):\n","                for p in layer.parameters():\n","                    p.requires_grad = trainable\n","            else:\n","                layer.requires_grad = trainable\n","\n","        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n","        print(f'{total_frozen_params=}')\n","\n","    def unfreeze_gpt_layers(self,):\n","        gpt_layers = [[\n","            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n","            self.transformer.h[i].attn,self.transformer.h[i].mlp\n","        ] for i in range(self.config.depth)]\n","        flatten = []\n","        for l in gpt_layers:\n","            flatten.extend(l)\n","\n","        for layer in flatten:\n","            if not isinstance(layer,nn.Parameter):\n","                for p in layer.parameters():\n","                    p.requires_grad = True\n","            else:\n","                layer.requires_grad = True\n","\n","    @classmethod\n","    def from_pretrained(self,config):\n","        model = VisionGPT2Model(config)\n","        sd = model.state_dict()\n","        keys = sd.keys()\n","        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n","        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n","        gpt_keys = [key for key in keys if key not in vit_keys]\n","\n","        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n","        sd_hf = gpt2_small.state_dict()\n","        hf_keys = sd_hf.keys()\n","        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n","        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","\n","        for k in hf_keys:\n","            if any(match in k for match in ignore_matches):\n","                continue\n","            if any(k.endswith(w) for w in transposed):\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        model.load_state_dict(sd)\n","\n","        return model\n","\n","    ###################### 이 부분 수정\n","    def embedding_images(self, videos): # videos : [batch_size, padded_frames, 3, 224, 224]\n","\n","      all_video_embeddings = []\n","\n","      for video in videos: # video : [padded_frames, channels, height, width]\n","\n","          one_video_embeddings = []\n","\n","          for frame in video: # frame : [channels, height, width]\n","\n","              frame = frame.unsqueeze(0)\n","              image = self.patch_embed(frame)\n","              image = self._pos_embed(image)\n","\n","              for i in range(self.config.depth):\n","                  embedded = self.blocks[i](image)\n","              one_video_embeddings.append(embedded)\n","\n","          one_video_embeddings_tensor = torch.stack(one_video_embeddings, dim = 0)\n","          mean_embedding = torch.mean(one_video_embeddings_tensor, dim = 0)\n","\n","          all_video_embeddings.append(mean_embedding)\n","\n","      all_video_embeddings_tensor = torch.cat(all_video_embeddings, dim = 0)\n","\n","      return all_video_embeddings_tensor # [batch_size, num_patches, embedding_dim]\n","\n","\n","    def forward(self, videos, input_ids, labels = None):\n","\n","        # print(f\"Videos shape: {videos.shape}\")  # [batch_size, 3, 224, 224]\n","        # print(f\"Input IDs shape: {input_ids.shape}\")  # [batch_size, seq_length]\n","\n","        # if labels is not None:\n","        #     print(f\"Labels shape: {labels.shape}\")  # [batch_size, seq_length]\n","\n","        video_embedding = self.embedding_images(videos)\n","        # print(f\"Video_embedding shape: {video_embedding.shape}\")\n","\n","        token_embeddings = self.transformer.wte(input_ids) # [batch_size, seq_len, embedding_dim]\n","        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n","        positional_embeddings = self.transformer.wpe(pos_embs)\n","        gpt_input = self.transformer.drop(token_embeddings + positional_embeddings)\n","\n","        # GPT 블록\n","        for i in range(self.config.depth): # 모델의 깊이만큼\n","            video_embedding = self.blocks[i](video_embedding)\n","            gpt_input = self.transformer.h[i](gpt_input, video_embedding) # i 번째 레이어 지나서\n","\n","        gpt_output = self.transformer.ln_f(gpt_input) # 최종 output\n","\n","        if labels is not None:\n","            lm_logits = self.lm_head(gpt_output)\n","            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n","            return loss\n","\n","        lm_logits = self.lm_head(gpt_output[:, [-1], :])\n","        return lm_logits\n","\n","    ###################### 이 부분 수정\n","\n","    def generate(self, frames, sequence, max_tokens=50, temperature=1.0, deterministic=False):\n","        for _ in range(max_tokens): # max_token 개수만큼 반복하면서 문장 생성\n","\n","            out = self(frames, sequence)\n","            out = out[:,-1,:] / temperature\n","            probs = F.softmax(out,dim=-1)\n","            if deterministic:\n","                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n","            else:\n","                next_token = torch.multinomial(probs,num_samples=1)\n","            sequence = torch.cat([sequence,next_token],dim=1)\n","            if next_token.item() == tokenizer.eos_token_id:\n","                break\n","\n","        return sequence.cpu().flatten()"]},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"Pbyq6TXldnDJ"}},{"cell_type":"markdown","metadata":{"id":"Qo3hh_Y2akqu"},"source":["#### Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcoNNmJYamSk"},"outputs":[],"source":["class Trainer:\n","    def __init__(self, model_config, train_config, dls):\n","\n","        self.train_config = train_config\n","        self.model_config = model_config\n","        self.device = self.train_config.device\n","\n","        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n","        self.model.pretrained_layers_trainable(trainable=False)\n","\n","        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n","\n","        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        self.scaler = GradScaler()\n","\n","        self.train_dl, self.val_dl = dls\n","\n","        total_steps = len(self.train_dl)\n","\n","        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n","        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n","            self.optim,\n","            max_lr=self.train_config.lr,\n","            epochs=self.train_config.epochs,\n","            steps_per_epoch=total_steps\n","        )\n","\n","#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n","\n","        self.metrics = pd.DataFrame()\n","        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n","\n","        self.gen_tfms = A.Compose([\n","            A.Resize(224,224),\n","            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n","            ToTensorV2()\n","        ])\n","\n","    ###################### 수정한 부분\n","\n","    def save_model(self, model_path = None):\n","        save_path = Path(model_path) if model_path is not None else self.train_config.model_path\n","        sd = self.model.state_dict()\n","        filename = f'captioner_best.pt'\n","        torch.save(sd, save_path / filename)\n","\n","\n","    def load_best_model(self,model_path = None):\n","        load_path = Path(model_path) if model_path is not None else self.train_config.model_path\n","        sd = torch.load(load_path/'captioner.pt')\n","        self.model.load_state_dict(sd)\n","\n","    def save_model_epoch(self, epoch, model_path = None):\n","        save_path = Path(model_path) if model_path is not None else self.train_config.model_path\n","        sd = self.model.state_dict()\n","        filename = f'captioner_epoch_{epoch}.pt'\n","        torch.save(sd, save_path / filename)\n","        print(f\"Epoch {epoch + 1} 완료. 모델 저장도 완료 :) \")\n","\n","    def train_one_epoch(self,epoch):\n","\n","        prog = tqdm(self.train_dl,total=len(self.train_dl))\n","\n","        running_loss = 0.\n","\n","        for image, input_ids, labels in prog:\n","\n","            with torch.amp.autocast('cuda'): ##\n","                image = image.to(self.device)\n","                input_ids = input_ids.to(self.device)\n","                labels = labels.to(self.device)\n","\n","                loss = self.model(image,input_ids,labels)\n","\n","                self.scaler.scale(loss).backward()\n","                self.scaler.step(self.optim)\n","                self.scaler.update()\n","                self.sched.step()\n","                self.optim.zero_grad(set_to_none=True)\n","\n","                running_loss += loss.item()\n","\n","                prog.set_description(f'train loss: {loss.item():.3f}')\n","\n","            del image, input_ids, labels, loss\n","\n","        train_loss = running_loss / len(self.train_dl)\n","        train_pxp = np.exp(train_loss)\n","\n","        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n","\n","\n","    @torch.no_grad()\n","    def valid_one_epoch(self,epoch):\n","\n","        prog = tqdm(self.val_dl,total=len(self.val_dl))\n","\n","        running_loss = 0.\n","\n","        for image, input_ids, labels in prog:\n","\n","            with autocast():\n","                image = image.to(self.device)\n","                input_ids = input_ids.to(self.device)\n","                labels = labels.to(self.device)\n","\n","                loss = self.model(image,input_ids,labels)\n","                running_loss += loss.item()\n","\n","                prog.set_description(f'valid loss: {loss.item():.3f}')\n","\n","            del image, input_ids, labels, loss\n","\n","        val_loss = running_loss / len(self.val_dl)\n","        val_pxp = np.exp(val_loss)\n","\n","        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n","\n","        return val_pxp\n","\n","\n","    def clean(self):\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","\n","    def fit(self,):\n","\n","        best_pxp = 1e9\n","        best_epoch = -1\n","        prog = tqdm(range(self.train_config.epochs))\n","\n","        for epoch in prog:\n","\n","            if epoch == self.train_config.freeze_epochs_gpt:\n","                self.model.unfreeze_gpt_layers()\n","                print('unfreezing GPT2 entirely...')\n","\n","            if epoch == self.train_config.freeze_epochs_all:\n","                self.model.pretrained_layers_trainable(trainable=True)\n","\n","            self.model.train()\n","            prog.set_description('training')\n","            self.train_one_epoch(epoch)\n","            self.clean()\n","\n","            self.model.eval()\n","            prog.set_description('validating')\n","            pxp = self.valid_one_epoch(epoch)\n","            self.clean()\n","\n","            print(self.metrics.tail(1))\n","\n","            if pxp < best_pxp:\n","                best_pxp = pxp\n","                best_epoch = epoch\n","                print('saving best model...')\n","                self.save_model(model_path = '/content/drive/MyDrive/Trained_Model')\n","\n","            self.save_model_epoch(epoch, model_path = '/content/drive/MyDrive/Trained_Model')\n","\n","        return {\n","            'best_perplexity': best_pxp,\n","            'best_epoch': best_epoch\n","        }\n","\n","    ###################### 수정한 부분\n","\n","    @torch.no_grad()\n","    def generate_caption(self, video_path, transform, max_tokens = 50,temperature = 1.0,deterministic = False):\n","\n","        self.model.eval()\n","\n","        frames = []\n","\n","        for frame_path in video_path: # 하나의 비디오 경로 (여러 프레임으로 이루어진)\n","            image = cv2.imread(frame_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = np.array(image, dtype = np.uint8)\n","            image = transform(image = image)[\"image\"]\n","            frames.append(image)\n","\n","        frames_tensor = torch.stack([torch.tensor(frame, dtype = torch.float32) for frame in frames])\n","        frames_tensor = frames_tensor.unsqueeze(0).to(self.device)\n","\n","        # sequence 초기화 (시작점) - bos_token_id 값을 가지는 [1, 1]의 텐서\n","        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n","\n","        caption = self.model.generate(\n","            frames_tensor,\n","            sequence,\n","            max_tokens = max_tokens,\n","            temperature = temperature,\n","            deterministic = deterministic\n","        )\n","        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n","\n","        return caption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv2dpFSEarl9"},"outputs":[],"source":["model_config = SimpleNamespace(\n","    vocab_size = 50_257,\n","    embed_dim = 768,\n","    num_heads = 12,\n","    seq_len = 1024,\n","    depth = 12,\n","    attention_dropout = 0.1,\n","    residual_dropout = 0.1,\n","    mlp_ratio = 4,\n","    mlp_dropout = 0.1,\n","    emb_dropout = 0.1,\n",")\n","train_config = SimpleNamespace(\n","    epochs = 3, ################### 기존은 5번\n","    freeze_epochs_gpt = 1,\n","    freeze_epochs_all = 2,\n","    lr = 1e-4,\n","    device = 'cuda',\n","    model_path = Path('captioner'),\n","    batch_size = 32\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fA7hrjPwk89n"},"outputs":[],"source":["import warnings\n","warnings.simplefilter(\"ignore\", UserWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfXsKNyzauNT"},"outputs":[],"source":["trainer = Trainer(model_config, train_config, (train_dataloader, val_dataloader))"]},{"cell_type":"markdown","source":["#### Train"],"metadata":{"id":"5NpyOSnadw3f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_O-M11kTavM1"},"outputs":[],"source":["trainer.fit()"]},{"cell_type":"markdown","metadata":{"id":"2Y3Nm8I2bFqX"},"source":["### Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2zroxEDbHzl"},"outputs":[],"source":["model_path = '/content/drive/MyDrive/Trained_Model'\n","\n","# best 모델 불러오기\n","trainer.load_best_model(model_path)"]},{"cell_type":"markdown","metadata":{"id":"RIrBiEe7b87e"},"source":["1. 분석하고 싶은 영상을 가져오기\n","2. 프레임으로 나누는 작업 진행\n","3. 결과 프레임들을 특정 경로(드라이브)에 저장\n","4. 프레임들 불러와서 new_frames_path 에 넣어주도록 generate_caption 함수를 조정했습니당\n","\n","정리) 최종 프레임들 경로가 필요합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrOYkDEQe1_U"},"outputs":[],"source":["# 경로들 정리\n","\n","## 최종 모델 저장, 불러올 때 - /content/drive/MyDrive/Trained_Model\n","## 테스트용 비디오 불러올 때 - /content/drive/MyDrive/Testing_Videos => \"testing_dir\"\n","## 프레임 저장은 Testing_Videos 안에 폴더 만들어서 저장 (영상 이름으로 폴더 만들어서)"]},{"cell_type":"markdown","metadata":{"id":"lrCQ_M0-0Omz"},"source":["#### Framing & Predicting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOhihoavclR-"},"outputs":[],"source":["### 프레임 추출 + 캡션 생성 - 준희 추가\n","\n","### 영상에서 프레임 추출\n","# video : 비디오 경로 / frames_dir : 프레임 추출해서 저장할 경로 / fps : 초당 추출할 프레임 수\n","\n","def extract_frames(video, frames_dir, fps):\n","  os.makedirs(frames_dir, exist_ok = True)\n","  command = f\"ffmpeg -i \\\"{video}\\\" -vf fps={fps} \\\"{frames_dir}/frame_%4d.jpg\\\"\"\n","  result = subprocess.run(command, shell=True, capture_output=True, text=True)\n","\n","  # 에러 발생시를 위한..\n","  if result.returncode != 0:\n","    print(f\"Error: {result.stderr}\")\n","\n","  return sorted([os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith(\".jpg\")])\n","\n","### 영상 캡션 생성\n","# frames_path : 프레임들 저장된 폴더\n","def predicting(frames_path, transform):\n","    frames = os.listdir(frames_path)\n","    new_frames_path = [os.path.join(frames_dir, frame) for frame in frames]\n","    new_caption = trainer.generate_caption(new_frames_path, transform, temperature = 1.0, deterministic = False)\n","    return new_caption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5sYJUqZkNdl"},"outputs":[],"source":["### 최종 함수\n","# video_name : 진짜 비디오 이름 / testing_dir : 영상 저장된 경로\n","\n","def framing_and_predicting(video_name, testing_dir, fps):\n","    # 경로 지정\n","    video_dir = os.path.join(testing_dir, f\"{video_name}.mp4\")\n","    frames_dir = os.path.join(testing_dir, f\"Frames/{video_name}\")\n","\n","    # 프레이밍, 저장\n","    frames = extract_frames(video_dir, frames_dir, fps = fps)\n","    print(f\"Extracted {len(frames)} frames.\")\n","\n","    new_caption = predicting(frames_dir, transform)\n","    return new_caption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOB4R7YPk4fV"},"outputs":[],"source":["# 일단 고양이 영상 하나에 테스팅\n","testing_dir = \"/content/drive/MyDrive/Testing_Videos/Videos\" # 영상이 저장된 경로\n","\n","framing_and_predicting(\"이비 와작\", testing_dir, fps = 2)"]},{"cell_type":"markdown","metadata":{"id":"pCRx0d280HBA"},"source":["#### Framing & Predicting & Visualizing the Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cDl0QWAypGR"},"outputs":[],"source":["def framing_and_predicting_and_visualizing(video_name, testing_dir, fps):\n","    # 경로 지정\n","    video_dir = os.path.join(testing_dir, f\"{video_name}.mp4\")\n","    frames_dir = os.path.join(testing_dir, f\"Frames/{video_name}\")\n","\n","    # 프레이밍, 저장\n","    frames = extract_frames(video_dir, frames_dir, fps = fps)\n","\n","    #########################################################\n","    # new_caption = predicting(frames_dir, transform)\n","    new_caption = \"a cute fluffy kitty\"\n","\n","    # 비디오 표시\n","    display(Video(video_dir, embed=True, width=640, height=480))\n","\n","    # 제목 텍스트 추가\n","    cap = cv2.VideoCapture(video_dir)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # 영상이랑 가로 길이 맞췄으면 좋겠어서....\n","\n","    plt.figure(figsize=(width / 90, 0.2), facecolor = 'black') # inch 기준은 96이라는데.. 흠\n","    plt.text(0.5, 0.5, f\"^._.^ {new_caption} ^._.^\", ha = 'center', va = 'center', color = 'white')\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRZQAlWKzRIH"},"outputs":[],"source":["### 캡션 생성하고 비디오 재생하기\n","testing_dir = \"/content/drive/MyDrive/Testing_Videos/Videos\" # 영상이 저장된 경로\n","\n","framing_and_predicting_and_visualizing(\"이비 와작\", testing_dir, fps = 2 )"]}]}