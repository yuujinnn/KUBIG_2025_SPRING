### python -m streamlit run app.py
### conda activate /Users/happykuma/miniconda3

import os
import requests
from bs4 import BeautifulSoup
import chromadb  # type: ignore
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction  # type: ignore
from openai import OpenAI  # type: ignore
from dotenv import load_dotenv
import streamlit as st

# í•œêµ­ ì‹œê°„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
import pytz

# naver api ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
import urllib.request
import json
import uuid
import re
import ssl
import urllib.parse
import numpy as np
import pandas as pd

# selenium ë¼ì´ë¸ŒëŸ¬ë¦¬ë¦¬
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


##### ë§ˆì§€ë§‰ì— ë³€ê²½í•˜ê¸° #####

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
api_key = os.getenv("OPENAI_API_KEY")

# ë„¤ì´ë²„ api ì„¤ì •
client_id = os.getenv("client_id")
client_secret = os.getenv("client_secret")

# ì˜¤ëŠ˜ ë‚ ì§œ ì„¤ì •
kst = pytz.timezone("Asia/Seoul")
today_kst = datetime.now(kst).strftime("%a, %d %b %Y")  # í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ í¬ë§·

# ëª¨ë¸ ì„¤ì •
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_MODEL = "gpt-4o-mini"

# OpenAI ë° ChromaDB ì´ˆê¸°í™”
embedding_function = OpenAIEmbeddingFunction(
    api_key=os.getenv("OPENAI_API_KEY"),
    model_name=EMBEDDING_MODEL
)

###################################
client = OpenAI()

chroma_client = chromadb.PersistentClient(path="./chroma_db")
corpus_collection = chroma_client.get_or_create_collection(
    name='NEWS',
    embedding_function=embedding_function
)

# RSS XMLì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
def fetch_news_titles(xml_url):
    response = requests.get(xml_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'lxml-xml')

    articles = []
    for item in soup.find_all('item', limit=5):  # ìµœì‹  ë‰´ìŠ¤ 5ê°œ ê°€ì ¸ì˜¤ê¸°
        title = item.find('title').text
        link = item.find('link').text
        articles.append((title, link))
    return articles

# í…ìŠ¤íŠ¸ chunk ë¶„í•  í•¨ìˆ˜ 
def smart_chunk_splitter(texts, titles, dates, max_chunk_size=1500):
    chunks = []

    if isinstance(texts, str):
        texts = [texts]
        titles = [titles]
        dates = [dates]

    for text, title, date in zip(texts, titles, dates):
        current_chunk = ""
        sentences = text.split('.')

        for sentence in sentences:
            sentence = sentence.strip()

            if not sentence:
                continue  # ë¹ˆ ë¬¸ì¥ì€ ìŠ¤í‚µ

            if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
                current_chunk += sentence + '. '

            else:
                current_chunk = f"title : {title}, date : {date}, content : {current_chunk}"
                chunks.append(current_chunk.strip())
                current_chunk = sentence + '. '

        if current_chunk:
            chunks.append(f"title: {title}, date: {date}, content: {current_chunk.strip()}")

    return chunks

# # í…ìŠ¤íŠ¸ chunk ë¶„í•  í•¨ìˆ˜ 2 (ì›ë³¸)
# def smart_chunk_splitter2(text, max_chunk_size=100):
#     sentences = text.split('. ')
#     chunks = []
#     current_chunk = ""

#     for sentence in sentences:
#         if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
#             current_chunk += sentence + '. '
#         else:
#             chunks.append(current_chunk.strip())
#             current_chunk = sentence + '. '

#     if current_chunk:
#         chunks.append(current_chunk.strip())
#     return chunks

# keyword ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ - 1
def create_chat_prompt1(user_query):
    system_prompt = """ ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì—ì„œ ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.

            ğŸ“Œ **ì§€ì¹¨**
            1. **ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´**ë¥¼ í¬ì°©í•œ í‚¤ì›Œë“œë¥¼ **ìµœëŒ€ 2ê°œê¹Œì§€** ì¶”ì¶œí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•˜ë‚˜ ì´ìƒ ì¶”ì¶œí•˜ì„¸ìš”.   
            2. **ë³µí•©ëª…ì‚¬**ëŠ” ê°€ëŠ¥í•œ í•œ **í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ** ì¶”ì¶œí•˜ì„¸ìš”.
            3. í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ **ë¬¸ì¥ ì•ˆì˜ ë‹¨ì–´**ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. 
            4. **ë¶ˆí•„ìš”í•œ ë‹¨ì–´(ì˜ˆ: "ê´€ë ¨", "ë‰´ìŠ¤", "ìš”ì•½", "ì •ë³´", "ì†Œì‹")ëŠ” ì œì™¸í•˜ì„¸ìš”.**
            5. ì¶œë ¥ í˜•ì‹: **í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„**í•´ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ ì ‘ë‘ì–´ ì—†ì´ ìˆœìˆ˜í•œ í‚¤ì›Œë“œë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
               ì˜ˆ) `ì†í¥ë¯¼ ê²½ê¸°`

            #Sentence: ì†í¥ë¯¼ ì„ ìˆ˜ê°€ ìµœê·¼ ê²½ê¸°ì—ì„œ ì–´ë–¤ ê²°ê³¼ë¥¼ ëƒˆëŠ”ì§€ ê¶ê¸ˆí•´.
            #Keyword: ì†í¥ë¯¼ ê²½ê¸°

            #Sentence: ìº¡í‹´ì•„ë©”ë¦¬ì¹´ ë³µì¥ì„ í•œ ì‚¬ëŒì´ êµ­íšŒì— ë‚œì…í–ˆë‹¤ê³  í•˜ëŠ”ë°, ê·¸ ì´ìœ ê°€ ë­ì•¼?
            #Keyword: ìº¡í‹´ì•„ë©”ë¦¬ì¹´ ë‚œì…
         
            #Sentence: ì‚¼ì„±, LG ê´€ë ¨ ë‰´ìŠ¤ ì•Œë ¤ì¤˜
            #Keyword: ì‚¼ì„± LG

            #Sentence: ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ì•Œë ¤ì¤˜
            #Keyword: ì˜¤ëŠ˜
            """
    return [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': f"""
            #Sentence: {user_query}
            #Keyword:
        """
        }
    ]

# ì¿¼ë¦¬ì—ì„œ keyword ì¶”ì¶œ
def generate_keywords(user_query):
    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=create_chat_prompt1(user_query),
        max_tokens=1500
    )
    return response.choices[0].message.content.replace("#Keyword:", "").strip() # type: ignore

# naver apië¡œ ë‰´ìŠ¤ ê²€ìƒ‰
def find_document(keyword, num_doc):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    def fetch_news(keyword, client_id, client_secret, num_doc):
        encText = urllib.parse.quote(keyword)
        url = f"https://openapi.naver.com/v1/search/news?query={encText}&display={num_doc}&sort=sim" # displayë¡œ ì˜¤íƒ€ ìˆ˜ì •

        context = ssl._create_unverified_context()

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)

        response = urllib.request.urlopen(request, context=context)
        if response.getcode() != 200:
            print("Error Code:", response.getcode())
            return []

        response_body = response.read().decode('utf-8')
        news_data = json.loads(response_body)

        return news_data.get('items', [])

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    news_items = fetch_news(keyword, client_id, client_secret, num_doc)

    contents = []
    titles = []
    dates = []
    url_pattern = re.compile(r"https://(m|n)\.([a-z]+\.)?naver\.com")

    # Selenium
    def get_mobile_news_content(driver, link):
        try:
            driver.get(link)
            title = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "NewsEndMain_article_head_title__ztaL4"))
            ).text
            content = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "_article_content"))
            ).text
            return title, content

        except Exception as e:
            print(f"Error loading mobile page: {link}, {e}")
            return "Failed to load content"

    # BeautifulSoup
    def get_desktop_news_content(link):
        try:
            page = requests.get(link, headers={"User-Agent": "Mozilla/5.0"})
            if page.status_code != 200:
                print(f"Failed to fetch page: {link} (Status Code: {page.status_code})")
                return "Failed to load content"

            soup = BeautifulSoup(page.content, "html.parser")
            title = soup.find(class_="media_end_head_headline").text.strip() # type: ignore
            article_body = soup.find("div", class_="newsct_article _article_body")
            content = article_body.get_text(strip=True) if article_body else "No content available"
            return title, content

        except Exception as e:
            print(f"Error processing link: {link}, {e}")
            return "Failed to load content"

    for item in news_items:
        link = item.get('link', 'No Link')
        date = item.get('pubDate', 'No Date')

        if not url_pattern.match(link):
            continue

        if link.startswith("https://m."):
            title, content= get_mobile_news_content(driver, link)

        else:
            title, content= get_desktop_news_content(link)

        # ì œëª©ì´ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì— ìˆìœ¼ë©´ ì¤‘ë³µì´ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        if title in titles:
            continue
        
        pattern = re.compile(r'(<.*?>|\(.*?ê¸°ì.*?\)|\[.*?ê¸°ì.*?\]|ë¬´ë‹¨ ì „ì¬.*?ê¸ˆì§€|â“’.*?\s|â–¶.*?\s|ì˜ìƒ|\[ì¶œì²˜.*?\]|í¬í† |ì•µì»¤|â–².*?\s)')
        cleaned_content = pattern.sub(' ', content)
        cleaned_content = re.sub(r'\s+', ' ', cleaned_content).strip()
        
        contents.append(cleaned_content)
        titles.append(title)
        dates.append(date)

    driver.quit()

    return contents, titles, dates

# QA í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ -2
def create_chat_prompt2(system_prompt, user_query, context_documents):
    # ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ í•´ì œ (ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”)
    if isinstance(context_documents[0], list):
        context_documents = [item for sublist in context_documents for item in sublist]

    return [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': f"""
            Context:
            {" ".join(context_documents)}

            ğŸ“Œ **ì§€ì¹¨**
            1. Contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            2. í•µì‹¬ ìš”ì ì„ ë¨¼ì € ì œì‹œí•œ í›„, ìƒì„¸í•œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.

            ğŸ“Œ **ì¶”ê°€ ë‰´ìŠ¤ ìš”ì²­ ì²˜ë¦¬**
            - "ë˜ ì—†ì–´?", "ë” ë³´ì—¬ì¤˜" ë“± ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ë©´ ì´ì „ ì§ˆë¬¸ì„ ì°¸ê³ í•´ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

            ğŸ“Œ **Contextì— ê´€ë ¨ ë‚´ìš©ì´ ì „í˜€ ì—†ì„ ì‹œì—ë§Œ, ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:**
            - "ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤." ë¬¸êµ¬ ì¶œë ¥
            - ë‹¹ì‹ ì´ ì•„ëŠ” ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            
            ğŸ“Œ **ì¶œì²˜ í‘œê¸°**
            ì¶œì²˜ê°€ ë˜ëŠ” ê¸°ì‚¬ì˜ ì œëª©ê³¼, ë³¸ë¬¸ ë¬¸ì¥ì„ ë‹µë³€ ë§ˆì§€ë§‰ì— "ì¶œì²˜:" ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”. ëª…í™•í•œ ë¬¸ì¥ì„ ë‚¨ê¸°ë©´ ì¢‹ìŠµë‹ˆë‹¤. 
            - ì—¬ëŸ¬ ì¶œì²˜ê°€ ìˆëŠ” ê²½ìš° ê°ê° ëª…ì‹œí•˜ì„¸ìš”.

            Question:
            {user_query}

            Answer(ì¶œì²˜ í¬í•¨):

        """} ### ì´ì „ ì§ˆë¬¸ ì–´ë–»ê²Œ ì°¸ê³ í•  ê²ƒ?
    ]

# ë‰´ìŠ¤ ìš”ì•½ í”„ë¡¬í”„íŠ¸
def create_headline_summary_prompt(headline_texts):
    return [
        {'role': 'system', 'content': "ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ë‰´ìŠ¤ ìš”ì•½ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."},
        {'role': 'user', 'content': f"""
            ì•„ë˜ì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ì •ë³´ë§Œ ë‚¨ê¸°ë„ë¡ ìš”ì•½í•˜ì„¸ìš”.
            ê° ë‰´ìŠ¤ëŠ” í•µì‹¬ ì‚¬ê±´, ì£¼ìš” ë°°ê²½, ì¸ê³¼ ê´€ê³„ë¥¼ í¬í•¨í•´ì•¼ í•˜ë©°, ìµœëŒ€ 3ì¤„ë¡œ ìš”ì•½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

            ìš”ì•½ í˜•ì‹:
            1. **ë‰´ìŠ¤ ì œëª©**  
               ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½
            2. **ë‰´ìŠ¤ ì œëª©**  
               ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½
            ...

            ë‰´ìŠ¤ë“¤:
            {headline_texts}

            ìš”ì•½:
        """}
    ]

# í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
def generate_headline_summary(headline_texts):
    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=create_headline_summary_prompt(headline_texts), 
        max_tokens=1500
    )
    return response.choices[0].message.content

    
# OpenAI APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(system_prompt, user_query, context_documents):
    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=create_chat_prompt2(system_prompt, user_query, context_documents),
        max_tokens=1500
    )
    return response.choices[0].message.content


# RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def chat_with_rag(system_prompt, user_query):
    query_result = corpus_collection.query(query_texts=[user_query], n_results=6) # 5ê°œì˜ chunkë¥¼ GPTì—ê²Œ ì „ë‹¬
    # ë””ë²„ê¹…ìš©
    # st.write(f"DB í¬ê¸° : {corpus_collection.count()}")
    
    # distance ì„ê³„ê°’ ì„¤ì •
    distances = query_result['distances'][0] # type: ignore
    threshold = 1.3

    # ë””ë²„ê¹…ìš©
    # st.write(f"ì„ê³„ê°’ : {threshold}") # ë””ë²„ê¹…ìš©
    # st.write(f"{distances}") # ë””ë²„ê¹…ìš©

    relevant_docs = [d for d in distances if d <= threshold]

    # ê´€ë ¨ ë¬¸ì„œê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    if len(relevant_docs) < 2:
        st.write("ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        keywords = generate_keywords(user_query)
        # ë””ë²„ê¹…ìš©
        # st.write(f"ğŸ” **í‚¤ì›Œë“œ:** {keywords}")  # í‚¤ì›Œë“œ í™•ì¸ - ë””ë²„ê¹…ìš©
        
        with st.spinner("ì›¹ì—ì„œ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
            news_list, news_title, news_date = find_document(keywords, num_doc=20)

            # naver_news ë‚´ìš© ì¶œë ¥ - ë””ë²„ê¹…ìš©
            # if news_list:
            #     st.write(f"ğŸ“Š **ê°€ì ¸ì˜¨ ë‰´ìŠ¤ ê°œìˆ˜:** {len(news_list)}")
            #     for idx, news in enumerate(news_list):
            #         st.write(f"**{idx + 1}.** {news[:100] if news else 'No content available'}...")
            # else:
            #     st.write("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            chunks = smart_chunk_splitter(news_list, news_title, news_date, max_chunk_size=1500) 
            
            # í˜„ì¬ ì €ì¥ëœ ë‰´ìŠ¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            existing_docs = set(corpus_collection.get()['documents']) # list -> setìœ¼ë¡œ ë³€í™˜ # type: ignore
            # ì¤‘ë³µ ì—†ëŠ” ë‰´ìŠ¤ë§Œ ì¶”ê°€
            new_chunks = [chunk for chunk in chunks if chunk not in existing_docs]

            # ê³ ìœ  ID ìƒì„± í•¨ìˆ˜ - ë””ë²„ê¹…ìš©
            def generate_unique_id():
                    return str(uuid.uuid4())

            # ìƒˆë¡œìš´ ë‰´ìŠ¤ DBì— ì¶”ê°€
            if new_chunks:
                try:
                    new_ids = [generate_unique_id() for _ in new_chunks]
                    corpus_collection.add(ids=new_ids, documents=new_chunks)

                    # ë””ë²„ê¹…ìš©
                    # st.success(f"âœ… {len(new_chunks)}ê°œì˜ ìƒˆë¡œìš´ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.") # ë””ë²„ê¹…ìš©
                    # st.write(f"ğŸ“Œ **DB í¬ê¸°:** {corpus_collection.count()}")
                    
                except Exception as e:
                    st.error(f"âŒ ë‰´ìŠ¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            else:
                st.info("ğŸ” ì¶”ê°€í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ì‚¬ìš©ì ì¿¼ë¦¬ì™€ DB ë‚´ contextë¥¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 6ê°œ ì¶”ì¶œ
            query_result = corpus_collection.query(query_texts=[keywords], n_results=6)
        ###################################
    response = generate_response(system_prompt, user_query, query_result['documents'])
    
    # ë””ë²„ê¹…ìš©
    # st.write(query_result['documents']) # ë””ë²„ê¹…ìš©

    return response


# **XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€**
def fetch_news_from_rss(rss_url, category_name="", summary = False):
    """RSSì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml-xml')

        items = soup.find_all('item')
        category_prefix = f"[ì˜¤ëŠ˜ì˜ {category_name} ë‰´ìŠ¤] " if category_name else ""
        
        # summary=Trueì¼ ë•Œë§Œ texts ìƒì„±
        texts = []
        if summary:
            texts = [
                f"ì˜¤ëŠ˜ì˜ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ {i+1}: {item.title.text}\n\n{item.find('content:encoded').text if item.find('content:encoded') else item.description.text}"
                for i, item in enumerate(items)
            ]

        # ë‰´ìŠ¤ ë°ì´í„° ì²­í¬ ë¦¬ìŠ¤íŠ¸
        chunks = [
            chunk
            for item in items
            for chunk in smart_chunk_splitter(
                item.find('content:encoded').text if item.find('content:encoded') else item.description.text,
                f"{category_prefix}, {item.title.text}", item.pubDate.text,
                max_chunk_size=1500
            )
        ]
        return (texts, chunks) if summary else chunks  # summary=Trueë©´ ë‘˜ ë‹¤ ë°˜í™˜, ì•„ë‹ˆë©´ chunksë§Œ ë°˜í™˜
    
    except requests.exceptions.RequestException as e:
        st.error(f"âŒ ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


###################################

# **ğŸ“Œ Streamlit UI ì„¤ì •**
st.set_page_config(page_title="NewSeans | AI ë‰´ìŠ¤ ì±—ë´‡", page_icon="ğŸ“°")
st.markdown('<p style="font-size:20px; margin-bottom: 0;">NewSeansì™€ í•¨ê»˜í•˜ëŠ” ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤</p>', unsafe_allow_html=True)
st.subheader("ë‰´ìŠ¤ ìš”ì•½ë¶€í„° ë¶„ì„ê¹Œì§€! ê¶ê¸ˆí•œ ê±´ ëª¨ë‘ ë‚˜ì—ê²Œ ë¬¼ì–´ë´ ğŸ¤–", divider=True)


# **ì„¸ì…˜ ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™” (ì±„íŒ… ê¸°ë¡ ì €ì¥)**
if "messages" not in st.session_state:
    st.session_state.messages = []

# **ì‚¬ì´ë“œë°”: ìµœì‹  ë‰´ìŠ¤ í‘œì‹œ**
st.sidebar.subheader("ğŸ”¥ ë‰´ìŠ¤ í—¤ë“œë¼ì¸")
rss_url = "https://www.yonhapnewstv.co.kr/category/news/headline/feed/"
news_articles = fetch_news_titles(rss_url)

# ì „ì²´ ìµœì‹  ë‰´ìŠ¤ í‘œì‹œ
for title, link in news_articles:
    st.sidebar.markdown(f"[{title}]({link})")

# **ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ RSS í”¼ë“œ**
news_categories = {
    "ìµœì‹ ": "http://www.yonhapnewstv.co.kr/browse/feed/",
    "ì •ì¹˜": "http://www.yonhapnewstv.co.kr/category/news/politics/feed/",
    "ê²½ì œ": "http://www.yonhapnewstv.co.kr/category/news/economy/feed/",
    "ì‚¬íšŒ": "http://www.yonhapnewstv.co.kr/category/news/society/feed/",
    "ì§€ì—­": "http://www.yonhapnewstv.co.kr/category/news/local/feed/",
    "ì„¸ê³„": "http://www.yonhapnewstv.co.kr/category/news/international/feed/",
    "ë¬¸í™”ã†ì—°ì˜ˆ": "http://www.yonhapnewstv.co.kr/category/news/culture/feed/",
    "ìŠ¤í¬ì¸ ": "http://www.yonhapnewstv.co.kr/category/news/sports/feed/"
}

# **ì¹´í…Œê³ ë¦¬ ì„ íƒ**
st.sidebar.subheader("ğŸ—‚ï¸ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬")

# ë¼ë””ì˜¤ ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ì„ íƒ (ê¸°ë³¸ê°’ ì—†ìŒ)
selected_category = st.sidebar.radio("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", list(news_categories.keys()), index=None)

# **ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°**
if selected_category:
    selected_url = news_categories[selected_category]
    category_articles = fetch_news_titles(selected_url)

    # ğŸ“Œ ì¹´í…Œê³ ë¦¬ ì œëª©ê³¼ ë‰´ìŠ¤ 3ê°œë¥¼ ë°”ë¡œ ì•„ë˜ì— í‘œì‹œ
    with st.sidebar.expander(f"ğŸ“Œ ì´ì‹œê° {selected_category} ë‰´ìŠ¤", expanded=True):
        for title, link in category_articles[:3]:
            st.markdown(f"- [{title}]({link})")


# # **ğŸ’¬ ì´ì „ ëŒ€í™”ë‚´ìš© í‘œì‹œ**
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.write(message["content"])

# âœ… ìµœì´ˆ í˜ì´ì§€ ë¡œë“œ ê°ì§€ (ì„¸ì…˜ ìœ ì§€)
if "page_loaded" not in st.session_state:
    st.session_state.page_loaded = False  # í˜ì´ì§€ê°€ ë¡œë“œë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ„
    st.session_state.corpus_collection = corpus_collection  # DB ìœ ì§€

# âœ… í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆì„ ë•Œë§Œ ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ & ì¶”ê°€
if not st.session_state.page_loaded:
    existing_ids = st.session_state.corpus_collection.get()["ids"]
    if existing_ids:
        st.session_state.corpus_collection.delete(ids=existing_ids)
        # ë””ë²„ê¹…ìš©
        # st.write(f"ğŸ—‘ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ! (ì‚­ì œëœ ë¬¸ì„œ ê°œìˆ˜: {len(existing_ids)})")

    # âœ… ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€ (í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰)
    headline_texts, chunks = fetch_news_from_rss(rss_url, "í—¤ë“œë¼ì¸", summary=True)

    # **ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€**
    for category, category_rss in news_categories.items():
        category_chunks = fetch_news_from_rss(category_rss, category)
        chunks.extend(category_chunks)  # chunk í•©ì¹˜ê¸°

    # **ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€ & ë¹ ë¥¸ ì²˜ë¦¬)**
    seen = set()
    new_chunks = [x for x in chunks if not (x in seen or seen.add(x))]

    # **ChromaDBì— ìƒˆ ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€**
    ids = [str(uuid.uuid4()) for _ in new_chunks]
    corpus_collection.add(ids=ids, documents=new_chunks)
    
    # ë””ë²„ê¹…ìš©
    # st.write(f"DB í¬ê¸° : {corpus_collection.count()}")
    # st.write(f"âœ… ìƒˆ ë‰´ìŠ¤ {len(new_chunks)}ê°œ ì¶”ê°€ ì™„ë£Œ!")

    # **í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜ í™•ì¸**
    existing_docs = corpus_collection.get()["documents"]
    
    # ë””ë²„ê¹…ìš©
    # st.write(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ë‰´ìŠ¤ ë¬¸ì„œ ê°œìˆ˜: {len(existing_docs)}") # type: ignore

    st.session_state.page_loaded = True  # ì´ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹˜

#####################
# ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
# ì²˜ìŒ ë“¤ì–´ê°”ì„ ë•Œë§Œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •
# í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìš”ì•½ ì¶œë ¥

if "headline_summary" not in st.session_state:
    with st.spinner('AIê°€ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ë¥¼ ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤...'):
        st.session_state.headline_summary = generate_headline_summary(headline_texts)

if "headline_title" not in st.session_state:
    st.session_state.headline_title = "ğŸ“° í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìš”ì•½"

if "headline_subtitle" not in st.session_state:
    st.session_state.headline_subtitle = "ì˜¤ëŠ˜ì˜ ì£¼ìš” í—¤ë“œë¼ì¸ ë‰´ìŠ¤ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:"

# í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìš”ì•½ ì œëª© ì¶œë ¥
st.header(st.session_state.headline_title)

# ì„¤ëª… ë¬¸êµ¬ ì¶œë ¥
st.write(st.session_state.headline_subtitle)

# ìš”ì•½ëœ ë‰´ìŠ¤ ì¶œë ¥
st.write(st.session_state.headline_summary)
#####################

# **ğŸ’¬ ì´ì „ ëŒ€í™”ë‚´ìš© í‘œì‹œ**
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# **ì‚¬ìš©ì ì…ë ¥ ì°½**
user_query = st.chat_input("ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”! ğŸ—£ï¸")

if user_query:
    # **ì‚¬ìš©ì ì§ˆë¬¸ì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥**
    st.session_state.messages.append({"role": "user", "content": user_query})

    with st.chat_message("user"):
        st.write(user_query)

    with st.spinner("â³ AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_prompt = """
        ìµœì‹  ë‰´ìŠ¤ ì§ˆë¬¸ì— ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ë‹¹ì‹ ì€ ì •í™•í•œ ë‚ ì§œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.

        ì˜¤ëŠ˜ ë‚ ì§œëŠ” {today_kst} ì…ë‹ˆë‹¤.
        ì•„ë˜ ì œê³µëœ ë¬¸ë§¥(Context)ì—ëŠ” ë¬¸ì„œì˜ ë‚ ì§œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        ë‹µë³€ì„ í•  ë•Œ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¥´ì„¸ìš”:
        1. **ì˜¤ëŠ˜(today)**, **ë‚´ì¼(tomorrow)**, **ì–´ì œ(yesterday)** ê°™ì€ ìƒëŒ€ì  ë‚ ì§œ í‘œí˜„ì„ ì •í™•íˆ í•´ì„í•˜ì„¸ìš”.
        2. ë¬¸ë§¥ì— í¬í•¨ëœ ë‚ ì§œ(date)ë¥¼ í˜„ì¬ ë‚ ì§œ({today_kst})ì™€ ë¹„êµí•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ë‚ ì§œ ì •ë³´ê°€ ì—†ì„ ê²½ìš° ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.
        """

        # RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        response_text = chat_with_rag(system_prompt, user_query)

        # **AI ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥**
        st.session_state.messages.append({"role": "assistant", "content": response_text})

    with st.chat_message("assistant"):
        st.write(response_text)