# -*- coding: utf-8 -*-
"""NLP_QA_extract_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158EPR_3bzu7hwRldLsnTsB2jLeTUSdYK
"""

!pip install transformers

!pip install sentence_transformers

!pip install huggingface_hub

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import os

import torch
print(torch.cuda.is_available())

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/헬스케어질의응답validationdata.zip -d /content

"""# 질문 키워드 추출"""

!huggingface-cli login # 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

import transformers
import torch

# Initialize the model
HF_API_TOKEN = "key"
model_name = "google/gemma-2b-it"

generator = transformers.pipeline(
    "text-generation",
    model=model_name,
    tokenizer=model_name,
    token=HF_API_TOKEN,
    device_map="auto",  # Automatically use GPU if available
    torch_dtype="auto"  # Uses float16 for faster generation
)

def extract_medical_keywords(current_query):

    prompt = (
        "You are a medical AI that specializes in extracting concise and meaningful keywords from medical-related user input. "
        "Your goal is to identify 4 core keywords that represent the most important medical terms, topics, or concepts within the input. "
        "These keywords should be specific and medically relevant, such as diseases, symptoms, definitions, or biological processes. "
        "Avoid overly broad or generic words like '아프다', '질병', '질문', '정보', '설명', or other non-medical terms.\n\n"
        f"The user is asking: '{current_query}'.\n"
        "You MUST extract exactly 4 korean keywords that are the most important medical terms or phrases representing the user's query. "
        "Output the keywords only as a comma-separated list. Do not include any explanations, extra text, or labels like 'Keywords:'."
    )

    # Generate response
    output = generator(
        prompt,
        max_new_tokens=20,
        truncation=True,
        do_sample=True,
        temperature=0.1
    )

    # Extract and clean the response
    keywords = output[0]["generated_text"].strip()

    return keywords

'''
def generate_embedding_average(keywords):
    # 각 키워드별로 임베딩 생성
    embs = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=keywords
    )

    # 각 키워드의 임베딩 벡터 추출
    embeddings = [emb.data.embedding for emb in embs.data]

    # 평균 벡터 계산
    average_embedding = np.mean(embeddings, axis=0)

    return average_embedding

def remove_stopwords(sentence, stopword_file_path):
    # 1. Stopword 리스트 불러오기
    with open(stopword_file_path, 'r', encoding='utf-8') as f:
        stopwords = set(f.read().strip().split('\n'))

    # 2. 단어 단위로 split하고 stopwords 제거
    words = re.findall(r'\b\w+\b', sentence)  # 한글 단어 추출
    filtered_words = [word for word in words if word not in stopwords]

    # 3. Stopword가 제거된 문장 생성
    cleaned_sentence = ' '.join(filtered_words)
    return cleaned_sentence

def embed_sentence_without_stopwords(sentence, stopword_file_path):
    # Stopword 제거
    cleaned_sentence = remove_stopwords(sentence, stopword_file_path)

    # KoSentenceBERT 모델 로드 (한국어 특화된 문장 임베딩 모델)
    model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

    # 문장 임베딩 생성
    embedding = model.encode(cleaned_sentence)
    return embedding
'''

extract_medical_keywords("비만이라는 질병의 정의와 특징을 알려주세요.")

"""# 답변 임베딩"""

!pip install langchain-community

import json
import os
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain.schema import Document

# 병합할 JSON 파일들이 위치한 최상위 디렉터리
base_dir = "/content/2.답변"  # 여러 개의 JSON 디렉토리가 들어있는 폴더 경로
output_file = "merged.json"  # 결과 JSON 파일명

# JSON 파일 읽어오기
def load_json(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

# 모든 JSON 파일을 찾고 병합
merged_data = []
for root, _, files in os.walk(base_dir):
    for file in files:
        if file.endswith(".json"):
            file_path = os.path.join(root, file)
            data = load_json(file_path)
            merged_data.append(data)  # 리스트에 추가

# 병합된 JSON 저장
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(merged_data, f, indent=4, ensure_ascii=False)

print(f"Merged JSON files from multiple directories into {output_file}")

# 병합된 JSON 파일 로드
data = load_json(output_file)

len(data)

print(data[200000])

# JSON 데이터를 Document 객체 리스트로 변환
documents = [Document(page_content=json.dumps(doc, ensure_ascii=False)) for doc in data]

documents[0]

pip install chromadb

# 벡터 DB 설정 및 임베딩
embedding_model_name = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"
embedding_model = HuggingFaceEmbeddings(
    model_name=embedding_model_name,
    encode_kwargs={"normalize_embeddings": True},
)

persist_directory = '/content/drive/MyDrive'

vectorstore = Chroma.from_documents(
    documents=documents, embedding=embedding_model, persist_directory = persist_directory #기본적으로 cosine유사도 사용
)

'''
vector_count = vectorstore.index.ntotal
print(f"저장된 벡터의 개수: {vector_count}")
'''

#vectorstore.save_local(persist_directory)

vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)

"""# 답변 생성"""

!pip install langchain_upstage

import os
import fast_main


os.environ["UPSTAGE_API_KEY"] = "key"

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def format_docs(docs):
    return '\n\n'.join([d.page_content for d in docs])

query = fast_main.user_input
docs = fast_main.custom_search_just_query(query, top_k=3)




# 최종 프롬프트 생성 함수
def generate_prompt(query, docs):
    prompt = f"""
    Context:
    You are an AI-powered healthcare assistant responsible for providing clear, accurate, and coherent medical information. Below are multiple relevant answers retrieved from a medical database that relate to the user's question.

    Your task is to **synthesize** these answers into **a single well-structured, natural, and cohesive response** in **Korean**. The response should read fluently and logically, as if written by a professional healthcare provider.

    **Guidelines:**
    1. **Avoid simply listing answers separately.** Instead, integrate them smoothly into a unified, well-organized response.
    2. **Ensure logical flow and coherence.** Organize the response so that it feels structured, with a natural progression of ideas.
    3. **Prioritize clarity and completeness.** Ensure the final response fully answers the user's question in an easy-to-understand yet professional manner.
    4. **Do not fabricate information.** Only use the provided medical answers to construct your response.

    ---

    User Question:
    {query}

    Relevant Answers:
    """
    for idx, doc in enumerate(docs, 1):
        metadata = doc.metadata
        prompt += f"\nAnswer {idx}:\nDepartment: {metadata.get('department_text', '없음')}\nDisease: {metadata.get('disease_kor_text', '없음')}\nIntention: {metadata.get('intention_text', '없음')}\nContent:\n"
        prompt += f"- {metadata.get('answer_text', '없음')}\n"

    prompt += "\nProvide a final response in Korean based on the above information."
    return prompt



prompt = generate_prompt(query, docs)

from langchain_upstage import ChatUpstage
# Model
llm = ChatUpstage()

# Chain
chain = prompt | llm | StrOutputParser()


# Run
response = chain.invoke({'context': (format_docs(docs)), 'question':query})
response